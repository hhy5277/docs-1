
  var css = require('sheetify')
  var minidocs = require('minidocs')
  var app = minidocs({"title":"Dat Data","logo":"dat-data-logo.svg","contents":[{"depth":1,"name":"Dat"},{"depth":2,"name":"Introduction","key":"dat","link":"/dat"},{"depth":2,"name":"How Dat Works","key":"how-dat-works","link":"/how-dat-works"},{"depth":2,"name":"FAQ","key":"faq","link":"/faq"},{"depth":1,"name":"Cookbook"},{"depth":2,"name":"Browser Dat","key":"browser","link":"/browser"},{"depth":2,"name":"DIY Dat","key":"diy-dat","link":"/diy-dat"},{"depth":1,"name":"Ecosystem"},{"depth":2,"name":"Overview","key":"ecosystem","link":"/ecosystem"},{"depth":2,"name":"SLEEP","key":"sleep","link":"/sleep"},{"depth":2,"name":"Hyperdrive","key":"hyperdrive","link":"/hyperdrive"},{"depth":2,"name":"Hypercore","key":"hypercore","link":"/hypercore"}],"markdown":"/Users/joe/node_modules/dat-docs/docs","initial":"dat","basedir":"","dir":"/Users/joe/node_modules/dat-docs","routes":{"index":"/","dat":"/dat/","how-dat-works":"/how-dat-works/","faq":"/faq/","browser":"/browser/","diy-dat":"/diy-dat/","ecosystem":"/ecosystem/","sleep":"/sleep/","hyperdrive":"/hyperdrive/","hypercore":"/hypercore/"},"html":{"dat":"<h1 id=\"dat\">Dat</h1>\n<p>Dat is a decentralized data tool for distributing data small and large.</p>\n<p><a href=\"http://webchat.freenode.net/?channels=dat\"><img src=\"https://img.shields.io/badge/irc%20channel-%23dat%20on%20freenode-blue.svg\" alt=\"#dat IRC channel on freenode\"></a>\n<a href=\"https://gitter.im/datproject/discussions?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\"><img src=\"https://badges.gitter.im/Join%20Chat.svg\" alt=\"datproject/discussions\"></a>\n<a href=\"http://docs.dat-data.com\"><img src=\"https://img.shields.io/badge/Dat%20Project-Docs-green.svg\" alt=\"docs\"></a></p>\n<h3 id=\"key-features-\">Key features:</h3>\n<ul>\n<li><strong>Live sync</strong> folders by sharing files as they are added or changed.</li>\n<li><strong>Distribute large files</strong> without copying data to a central server by connecting directly to peers.</li>\n<li><strong>Intelligently sync</strong> by deduplicating data between versions.</li>\n<li><strong>Verify data integrity</strong> using strong cryptographic hashes.</li>\n<li><strong>Work everywhere</strong>, including on the <a href=\"https://github.com/datproject/dat\">command line</a>, in the <a href=\"https://github.com/datproject/dat.land\">browser</a>, and on the <a href=\"https://github.com/juliangruber/dat-desktop\">desktop</a>.</li>\n</ul>\n<h3 id=\"-documentation-http-docs-dat-data-com-video-demo-https-www-youtube-com-watch-v-fxkjsycoqo4-ecosystem-https-github-com-clkao-awesome-dat-\"><a href=\"http://docs.dat-data.com\">Documentation</a> | <a href=\"https://www.youtube.com/watch?v=fxKjSyCoqO4\">Video Demo</a> | <a href=\"https://github.com/clkao/awesome-dat\">Ecosystem</a></h3>\n<hr>\n<h2 id=\"dat-command-line-tool\">Dat Command Line Tool</h2>\n<p>This guide will help you get started with the Dat command line tool. We are also developing <a href=\"https://github.com/datproject/dat.land\">web</a> and <a href=\"https://github.com/juliangruber/dat-desktop\">desktop</a> applications for Dat.</p>\n<h3 id=\"table-of-contents\">Table of Contents</h3>\n<li><a href=\"#getting-started\">Getting Started</a></li>\n<li><a href=\"#using-dat\">Using Dat</a></li>\n<li><a href=\"#troubleshooting\">Troubleshooting</a></li>\n<li><a href=\"#for-developers\">For Developers</a></li>\n\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>The Dat command line tool can be used to share, download, and sync files across many computers via the command line.</p>\n<table>\n<thead>\n<tr>\n<th>Windows</th>\n<th>Mac/Linux</th>\n<th>Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://ci.appveyor.com/project/datproject/dat\"><img src=\"https://ci.appveyor.com/api/projects/status/github/datproject/dat?branch=master&amp;svg=true\" alt=\"Build status\"></a></td>\n<td><a href=\"https://travis-ci.org/datproject/dat\"><img src=\"https://api.travis-ci.org/datproject/dat.svg\" alt=\"Travis\"></a></td>\n<td><a href=\"https://npmjs.org/package/dat\"><img src=\"https://img.shields.io/npm/v/dat.svg?style=flat-square\" alt=\"NPM version\"></a></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"installation\">Installation</h3>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> -g dat\n</code></pre><p>You should be able to run the <code>dat</code> command now. If not, see the <a href=\"#troubleshooting\">installation troubleshooting</a> for tips.</p>\n<h3 id=\"demo\">Demo</h3>\n<p>We have Dat installed, now let&#39;s use it! In this quick demo we will download our documentation files using Dat.</p>\n<p>You tell Dat what files to download by giving it a Dat link. Dat links are 64 character strings, for example <code>395e3467bb5b2fa083ee8a4a17a706c5574b740b5e1be6efd65754d4ab7328c2</code>.</p>\n<p>Along with the link, you tell Dat where to download the files. All together, you can download files by typing <code>dat &lt;dat-link&gt; &lt;download-directory&gt;</code>.</p>\n<p>We have our Dat documentation folders being shared by Dat (at the key above). For this example, we can download those files to your computer. In your console, run:</p>\n<pre><code>dat <span class=\"hljs-number\">395e3467</span>bb5b2fa083ee8a4a17a706c5574b740b5e1be6efd65754d4ab7328c2 dat_docs\n</code></pre><p>You should see the output below. Once the download is finished, the files will be available on your computer in the <code>dat_docs</code> folder!</p>\n<p><img src=\"https://raw.githubusercontent.com/datproject/docs/master/assets/cli_download.gif\" alt=\"Download gif\"></p>\n<h3 id=\"cli-development-status\">CLI Development Status</h3>\n<p>This is the Dat CLI 1.0 release candidate (RC2). We are actively seeking feedback &amp; developing this release candidate. Follow <a href=\"https://github.com/datproject/projects/issues/5\">this issue</a> for the Dat CLI road map discussion and see <a href=\"https://github.com/datproject/dat/issues/486\">known RC2 issues</a>.</p>\n<p><strong>Please note</strong> that previous versions of Dat (alpha, beta) are incompatible with the 1.0 release candidate.</p>\n<h2 id=\"using-dat\">Using Dat</h2>\n<p>There are two commands in Dat:</p>\n<ol>\n<li>Share data: <code>dat &lt;directory&gt;</code> will share a directory on your computer.</li>\n<li>Download data: <code>dat &lt;dat-link&gt; &lt;download-directory&gt;</code> will download files from the Dat link to a directory on your computer. </li>\n</ol>\n<p>Running <code>dat</code> in the console, with no arguments, will show you the usage guide. You can always use this as a reference for all the commands:</p>\n<pre><code>dat &lt;directory&gt;\n\n  share directory and create a dat-link\n\n  -<span class=\"ruby\">-snapshot            create a snapshot of directory\n</span>  -<span class=\"ruby\">-port, -p            set a specific inbound tcp port\n</span>\ndat &lt;dat-link&gt; &lt;directory&gt;\n\n  download a dat-link into directory\n\n  -<span class=\"ruby\">-exit                exit process after download finishes\n</span>  -<span class=\"ruby\">-port, -p            set a specific inbound tcp port\n</span>\ngeneral options\n\n  -<span class=\"ruby\">-version, -v         get installed dat version\n</span>  -<span class=\"ruby\">-temp                use <span class=\"hljs-keyword\">in</span>-memory database instead of .dat folder\n</span>  -<span class=\"ruby\">-webrtc              connect to webrtc peers via electron-webrtc\n</span>  -<span class=\"ruby\">-doctor              run dat doctor\n</span>  -<span class=\"ruby\">-quiet, -q           output only dat-link, no progress information\n</span>  -<span class=\"ruby\">-debug               show debugging output\n</span>  -<span class=\"ruby\">-ignore-hidden       ignore hidden files (<span class=\"hljs-literal\">true</span> by default)</span>\n</code></pre><h3 id=\"sharing-files\">Sharing Files</h3>\n<p>Share a directory by typing <code>dat &lt;directory&gt;</code>:</p>\n<pre><code>$ dat my_data/\nSharing /Users/joe/my_data/\n\nShare Link: d6e1875598fae25165eff440ffd01513197ad0db9dbb9898f2a141288b9322c6\nThe Share Link <span class=\"hljs-keyword\">is</span> secret <span class=\"hljs-built_in\">and</span> <span class=\"hljs-keyword\">only</span> those you share it with will <span class=\"hljs-keyword\">be</span> able <span class=\"hljs-keyword\">to</span> <span class=\"hljs-built_in\">get</span> the <span class=\"hljs-keyword\">files</span>\n\n[==============&gt;] Added <span class=\"hljs-number\">2</span> <span class=\"hljs-keyword\">files</span> (<span class=\"hljs-number\">1.44</span> kB/<span class=\"hljs-number\">1.44</span> kB)\n\nConnected <span class=\"hljs-keyword\">to</span> <span class=\"hljs-number\">1</span> peers. Uploading <span class=\"hljs-number\">288.2</span> B/s. Watching <span class=\"hljs-keyword\">for</span> updates...\n</code></pre><p>You are now publishing that data from your computer. It will be publicly accessible as long as your terminal is open and the process is still running. The hash is a <strong>secret hash</strong>, your data is visible to anyone you send the hash to.</p>\n<h4 id=\"updating-shared-files\">Updating Shared Files</h4>\n<p>Dat makes it easy to share a folder and send files as they are added to the folder. By default, when you share using <code>dat my_data/</code> you will be in live sync mode. Anyone connected to you will receive new files.</p>\n<h4 id=\"creating-a-snapshot\">Creating a snapshot</h4>\n<p>A snapshot reads the files and creates a unique link that will always be the same for that set of files (if they remain unchanged). To create a snapshot use the snapshot option: <code>dat my_data/ --snapshot</code>. Snapshots are automatically created for you in live mode as files update.</p>\n<h4 id=\"sharing-options\">Sharing Options</h4>\n<p><code>dat &lt;directory&gt; --snapshot</code></p>\n<p>Share a snapshot of the current files. </p>\n<p><code>dat &lt;directory&gt; --port=1234</code></p>\n<p>Set your inbound TCP port. This is useful for debugging or on restrictive networks. </p>\n<h3 id=\"downloading-files\">Downloading Files</h3>\n<p>Download files from a Dat link by typing: <code>dat &lt;dat-link&gt; &lt;download-directory&gt;</code>:</p>\n<pre><code>$ dat d6e1875598fae25165eff440ffd01513197ad0db9dbb9898f2a141288b9322c6 download_dir\nDownloading in /Users/joe/download_dir\n\nShare Link: d6e1875598fae25165eff440ffd01513197ad0db9dbb9898f2a141288b9322c6\nThe Share Link <span class=\"hljs-keyword\">is</span> secret <span class=\"hljs-built_in\">and</span> <span class=\"hljs-keyword\">only</span> those you share it with will <span class=\"hljs-keyword\">be</span> able <span class=\"hljs-keyword\">to</span> <span class=\"hljs-built_in\">get</span> the <span class=\"hljs-keyword\">files</span>\n\n[==============&gt;] Downloaded <span class=\"hljs-number\">3</span> <span class=\"hljs-keyword\">files</span> (<span class=\"hljs-number\">1.44</span> kB/<span class=\"hljs-number\">1.44</span> kB)\n\nConnected <span class=\"hljs-keyword\">to</span> <span class=\"hljs-number\">1</span> peers. Downloading <span class=\"hljs-number\">1.44</span> kB/s. Watching <span class=\"hljs-keyword\">for</span> updates...\n</code></pre><p>Dat will start downloading the data into the <code>download_dir</code> folder. Once the download is finished (a message will print and the bar will turn green), you can safely exit the process with <code>Ctrl-C</code> (<code>Cmd-C</code> on Mac). </p>\n<p>While downloading, you may be connected to more than 1 peer. Anyone who has the Dat link will be able to download and re-host a copy of the data. So you may be downloading from (and sharing to) other people that are also downloading that data! You only need one block of data to start helping as a host. It&#39;s distributed mad science!</p>\n<h4 id=\"updating-the-downloaded-files\">Updating the Downloaded Files</h4>\n<p>What happens if the files get updated? Dat auto-syncs new files if it is still running. If you exited the process, you can run the same command you ran before (with the same link and directory) and the files will update!</p>\n<h4 id=\"download-options\">Download Options</h4>\n<p><code>dat &lt;dat-link&gt; &lt;directory&gt; --exit</code> </p>\n<p>After files are done downloading, exit the process. If you are connected to a live Dat you will not get new files unless you run the command again.</p>\n<p><code>dat &lt;dat-link&gt; &lt;directory&gt; --port=1234</code></p>\n<p>Set your inbound TCP port. This is useful for debugging or on restrictive networks. </p>\n<h3 id=\"live-sync-snapshots\">Live Sync &amp; Snapshots</h3>\n<p>Dat makes it easy to share a folder and send files as they are changed or added. By default, when you share using Dat you will be in <em>live sync</em> mode. Anyone connected to you will receive file changes as you make them.</p>\n<p>When downloading a Dat, you do not have to worry about live mode. It will automatically start in the right mode based on the link. </p>\n<p>To create a snapshot when sharing files use the snapshot option: <code>dat my_data/ --snapshot</code>. A snapshot reads the files and creates a specific link that will never change (as long as the files don&#39;t change).</p>\n<h3 id=\"dat-metadata-storage\">Dat Metadata Storage</h3>\n<p>When you run a command, Dat creates a hidden folder, <code>.dat</code>, in the directory specified. Similar to git, this folder stores information about your Dat. File metadata and the Dat link are stored to make it easy to continue sharing or downloading the same directory.</p>\n<h4 id=\"temporary-database\">Temporary Database</h4>\n<p>Use the <code>--temp</code> option to keep the metadata in memory instead of the <code>.dat</code> folder.</p>\n<h3 id=\"sharing-or-download-files-with-dat-land-http-dat-land-\">Sharing or Download files with <a href=\"http://dat.land\">dat.land</a></h3>\n<p>You can use the <code>--webrtc</code> option to share files to dat.land. You&#39;ll need to install <code>electron-webrtc</code> in order for this to work.</p>\n<h2 id=\"troubleshooting\">Troubleshooting</h2>\n<p>We&#39;ve provided some troubleshooting tips based on issues users have seen. Please <a href=\"https://github.com/datproject/dat/issues/new\">open an issue</a> or ask us in our <a href=\"https://gitter.im/datproject/discussions\">chat room</a> if you need help troubleshooting and it is not covered here.</p>\n<p>If you have trouble sharing/downloading in a directory with a <code>.dat</code> folder, try deleting it and running the command again.</p>\n<h4 id=\"check-your-dat-version\">Check Your Dat Version</h4>\n<p>Knowing the version is really helpful if you run into any bugs, and will help us troubleshoot your issue.</p>\n<p>Check your Dat version:</p>\n<pre><code><span class=\"hljs-attribute\">dat -v</span>\n</code></pre><p>You should see the Dat semantic version printed, e.g. 11.1.2.</p>\n<h3 id=\"installation-issues\">Installation Issues</h3>\n<h4 id=\"node-npm\">Node &amp; npm</h4>\n<p>To use the Dat command line tool you will need to have <a href=\"https://docs.npmjs.com/getting-started/installing-node\">node and npm installed</a>. Make sure those are installed correctly before installing Dat. You can check the version of each:</p>\n<pre><code><span class=\"hljs-keyword\">node</span> <span class=\"hljs-title\">-v</span>\nnpm -v\n</code></pre><h4 id=\"global-install\">Global Install</h4>\n<p>The <code>-g</code> option installs Dat globally allowing you to run it as a command. Make sure you installed with that option.</p>\n<ul>\n<li>If you receive an <code>EACCES</code> error, read <a href=\"https://docs.npmjs.com/getting-started/fixing-npm-permissions\">this guide</a> on fixing npm permissions.</li>\n<li>If you receive an <code>EACCES</code> error, you may also install dat with sudo: <code>sudo npm install -g dat</code>.</li>\n<li>Have other installation issues? Let us know, you can <a href=\"https://github.com/datproject/dat/issues/new\">open an issue</a> or ask us in our <a href=\"https://gitter.im/datproject/discussions\">chat room</a>.</li>\n</ul>\n<h3 id=\"networking-issues\">Networking Issues</h3>\n<p>Networking capabilities vary widely with each computer, network, and configuration. Whenever you run a Dat there are several steps to share or download files with peers:</p>\n<ol>\n<li>Discovering Peers</li>\n<li>Connecting to Peers</li>\n<li>Sending &amp; Receiving Data</li>\n</ol>\n<p>With successful use, Dat will show <code>Connected to 1 peer</code> after connection. If you never see a peer connected your network may be restricting discovery or connection. Please try using the <code>dat --doctor</code> command (see below) between the two computers not connecting. This will help troubleshoot the networks.</p>\n<ul>\n<li>Dat may <a href=\"https://github.com/datproject/dat/issues/503\">have issues</a> connecting if you are using iptables.</li>\n</ul>\n<h4 id=\"dat-doctor\">Dat Doctor</h4>\n<p>We&#39;ve included a tool to identify network issues with Dat, the Dat doctor. You will need to run the command on both the computers you are trying to share data between. On the first computer, run:</p>\n<pre><code>dat <span class=\"hljs-comment\">--doctor</span>\n</code></pre><p>The doctor will print out a command to run on the other computer, <code>dat --doctor=&lt;64-character-string&gt;</code>. The doctor will run through the key steps in the process of sharing data between computers to help identify the issue.</p>\n<hr>\n<h2 id=\"for-developers\">For Developers</h2>\n<p>Please see <a href=\"https://github.com/datproject/dat/blob/master/CONTRIBUTING.md\">guidelines on contributing</a> before submitting an issue or PR.</p>\n<h3 id=\"installing-from-source\">Installing from source</h3>\n<p>Clone this repository and in a terminal inside of the folder you cloned run this command:</p>\n<pre><code><span class=\"hljs-built_in\">npm</span> link\n</code></pre><p>This should add a <code>dat</code> command line command to your PATH. Now you can run the <code>dat</code> command to try it out.</p>\n<p>The contribution guide also has more tips on our <a href=\"https://github.com/datproject/dat/blob/master/CONTRIBUTING.md#development-workflow\">development workflow</a>.</p>\n<h3 id=\"internal-api\">Internal API</h3>\n<p><strong>Note: we are in the process of moving the js library to a separate module, <a href=\"https://github.com/joehand/dat-js\">joehand/dat-js</a>.</strong></p>\n<h4 id=\"dat-download-cb-\">dat.download(cb)</h4>\n<p>download <code>dat.key</code> to <code>dat.dir</code></p>\n<h4 id=\"dat-share-cb-\">dat.share(cb)</h4>\n<p>share directory specified in <code>opts.dir</code></p>\n<p>Swarm is automatically joined for key when it is available for share &amp; download (<code>dat.joinSwarm()</code>).</p>\n<h4 id=\"events\">Events</h4>\n<h5 id=\"initialization\">Initialization</h5>\n<ul>\n<li><code>dat.on(&#39;ready&#39;)</code>: db created/read &amp; hyperdrive archive created.</li>\n<li><code>dat.on(&#39;error&#39;)</code>: init/database error</li>\n</ul>\n<h5 id=\"swarm\">Swarm</h5>\n<p>Swarm events and stats are available from <code>dat.swarm</code>.</p>\n<ul>\n<li><code>dat.on(&#39;connecting&#39;)</code>: looking for peers</li>\n<li><code>dat.on(&#39;swarm-update&#39;)</code>: peer number changed</li>\n</ul>\n<h5 id=\"share\">Share</h5>\n<ul>\n<li><code>dat.on(&#39;key&#39;)</code>: key is available (this is at archive-finalized for snapshots)</li>\n<li><code>dat.on(&#39;append-ready&#39;)</code>: file count available (<code>dat.appendStats</code>), about to start appending to hyperdrive</li>\n<li><code>dat.on(&#39;file-added&#39;)</code>: file added to archive</li>\n<li><code>dat.on(&#39;upload&#39;, data)</code>: piece of data uploaded</li>\n<li><code>dat.on(&#39;archive-finalized&#39;)</code>: archive finalized, all files appended</li>\n<li><code>dat.on(&#39;archive-updated&#39;)</code>: live archive changed</li>\n</ul>\n<h5 id=\"download\">Download</h5>\n<ul>\n<li><code>dat.on(&#39;key&#39;)</code>: key is available</li>\n<li><code>dat.on(&#39;file-downloaded&#39;, file)</code>: file downloaded</li>\n<li><code>dat.on(&#39;download&#39;, data)</code>: piece of data downloaded</li>\n<li><code>dat.on(&#39;upload&#39;, data)</code>: piece of data uploaded</li>\n<li><code>dat.on(&#39;download-finished&#39;)</code>: archive download finished</li>\n</ul>\n<h4 id=\"other-api\">Other API</h4>\n<ul>\n<li><code>dat.key</code>: key</li>\n<li><code>dat.dir</code>: directory</li>\n<li><code>dat.datPath</code>: path to .dat folder</li>\n<li><code>dat.db</code>: database instance</li>\n<li><code>dat.swarm</code>: hyperdrive-archive-swarm instance</li>\n<li><code>dat.archive</code>: hyperdrive archive</li>\n<li><code>dat.snapshot</code> (boolean): sharing snapshot archive</li>\n</ul>\n<h4 id=\"internal-stats\">Internal Stats</h4>\n<pre><code class=\"lang-javascript\">\ndat.stats = {\n    <span class=\"hljs-attr\">filesTotal</span>: <span class=\"hljs-number\">0</span>, <span class=\"hljs-comment\">// Latest archive size</span>\n    bytesTotal: <span class=\"hljs-number\">0</span>,\n    <span class=\"hljs-attr\">bytesUp</span>: <span class=\"hljs-number\">0</span>,\n    <span class=\"hljs-attr\">bytesDown</span>: <span class=\"hljs-number\">0</span>,\n    <span class=\"hljs-attr\">rateUp</span>: speedometer(),\n    <span class=\"hljs-attr\">rateDown</span>: speedometer()\n}\n\n<span class=\"hljs-comment\">// Calculated on share before append starts. Used for append progress.</span>\n<span class=\"hljs-comment\">// Not updated for live.</span>\ndat.appendStats = {\n    <span class=\"hljs-attr\">files</span>: <span class=\"hljs-number\">0</span>,\n    <span class=\"hljs-attr\">bytes</span>: <span class=\"hljs-number\">0</span>,\n    <span class=\"hljs-attr\">dirs</span>: <span class=\"hljs-number\">0</span>\n}\n</code></pre>\n<h2 id=\"license\">License</h2>\n<p>BSD-3-Clause</p>\n","how-dat-works":"<h1 id=\"how-dat-works\">How Dat Works</h1>\n<p>Note this is about Dat 1.0 and later. For historical info about earlier incarnations of Dat (Alpha, Beta) check out <a href=\"http://dat-data.com/blog/2016-01-19-brief-history-of-dat\">this post</a>.</p>\n<p>When someone starts downloading data with the <a href=\"https://github.com/datproject/dat\">Dat command-line tool</a>, here&#39;s what happens:</p>\n<h2 id=\"phase-1-source-discovery\">Phase 1: Source discovery</h2>\n<p>Dat links look like this: <code>dat.land/c3fcbcdcf03360529b47df32ccfb9bc1d7f64aaaa41cca43ca9ac7f6778db8da</code>. The domain, dat.land, is there so if someone opens the link in a browser we can provide them with download instructions, and as an easy way for people to visually distinguish and remember Dat links. Dat itself doesn&#39;t actually use the dat.land part, it just needs the last part of the link which is a fingerprint of the data that is being shared. The first thing that happens when you go to download data using one of these links is you ask various discovery networks if they can tell you where to find sources that have a copy of the data you need.</p>\n<p>Source discovery means finding the IP and port of all the known data sources online that have a copy of that data you are looking for. You can then connect to them and begin exchanging data. By introducing this discovery phase we are able to create a network where data can be discovered even if the original data source disappears.</p>\n<p>The discovery protocols we use are <a href=\"https://en.wikipedia.org/wiki/Name_server\">DNS name servers</a>, <a href=\"https://en.wikipedia.org/wiki/Multicast_DNS\">Multicast DNS</a> and the <a href=\"https://en.wikipedia.org/wiki/Mainline_DHT\">Kademlia Mainline Distributed Hash Table</a> (DHT). Each one has pros and cons, so we combine all three to increase the speed and reliability of discovering data sources.</p>\n<p>We run a <a href=\"https://www.npmjs.com/package/dns-discovery\">custom DNS server</a> that Dat clients use (in addition to specifying their own if they need to), as well as a <a href=\"https://github.com/bittorrent/bootstrap-dht\">DHT bootstrap</a> server. These discovery servers are the only centralized infrastructure we need for Dat to work over the Internet, but they are redundant, interchangeable, never see the actual data being shared, and anyone can run their own and Dat will still work even if they all go down. If this happens discovery will just be manual (e.g. manually sharing IP/ports). Every data source that has a copy of the data also advertises themselves across these discovery networks.</p>\n<p>The discovery logic itself is handled by a module that we wrote called <a href=\"http://npmjs.org/discovery-channel\">discovery-channel</a>, which wraps other modules we wrote to implement DNS and DHT logic into a single interface. We can give the Dat link we want to download to discovery-channel and we will get back all the sources it finds across the various discovery networks.</p>\n<h2 id=\"phase-2-source-connections\">Phase 2: Source connections</h2>\n<p>Up until this point we have just done searches to find who has the data we need. Now that we know who should talk to, we have to connect to them. We use either <a href=\"https://en.wikipedia.org/wiki/Transmission_Control_Protocol\">TCP</a> or <a href=\"https://en.wikipedia.org/wiki/Micro_Transport_Protocol\">UTP</a> sockets for the actual peer to peer connections. UTP is nice because it is designed to <em>not</em> take up all available bandwidth on a network (e.g. so that other people sharing your wifi can still use the Internet). We then layer on our own file sharing protocol on top, called <a href=\"https://github.com/mafintosh/hypercore\">Hypercore</a>. We also are working on WebRTC support so we can incorporate Browser and Electron clients for some really open web use cases.</p>\n<p>When we get the IP and port for a potential source we try to connect using all available protocols (currently TCP and sometimes UTP) and hope one works. If one connects first, we abort the other ones. If none connect, we try again until we decide that source is offline or unavailable to use and we stop trying to connect to them. Sources we are able to connect to go into a list of known good sources, so that if our Internet connection goes down we can use that list to reconnect to our good sources again quickly.</p>\n<p>If we get a lot of potential sources we pick a handful at random to try and connect to and keep the rest around as additional sources to use later in case we decide we need more sources. A lot of these are parameters that we can tune for different scenarios later, but have started with some best guesses as defaults.</p>\n<p>The connection logic is implemented in a module called <a href=\"https://www.npmjs.com/package/discovery-swarm\">discovery-swarm</a>. This builds on discovery-channel and adds connection establishment, management and statistics. You can see stats like how many sources are currently connected, how many good and bad behaving sources you&#39;ve talked to, and it automatically handles connecting and reconnecting to sources for you. Our UTP support is implemented in the module <a href=\"https://www.npmjs.com/package/utp-native\">utp-native</a>.</p>\n<h2 id=\"phase-3-data-exchange\">Phase 3: Data exchange</h2>\n<p>So now we have found data sources, have connected to them, but we havent yet figured out if they <em>actually</em> have the data we need. This is where our file transfer protocol <a href=\"https://www.npmjs.com/package/hyperdrive\">Hyperdrive</a> comes in.</p>\n<p>The short version of how Hyperdrive works is: It breaks file contents up in to pieces, hashes each piece and then constructs a <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle tree</a> out of all of the pieces. This ultimately gives us the Dat link, which is the top level hash of the Merkle tree.</p>\n<p>Here&#39;s the long version:</p>\n<p>Hyperdrive shares and synchronizes a set of files, similar to rsync or Dropbox. For each file in the drive we use a technique called Rabin fingerprinting to break the file up into pieces. Rabin fingerprints are a specific strategy for what is called Content Defined Chunking. Here&#39;s an example:</p>\n<p><img src=\"https://raw.githubusercontent.com/datproject/docs/master/assets/cdc.png\" alt=\"cdc diagram\"></p>\n<p>We have configured our Rabin chunker to produce chunks that are around 16KB on average. So if you share a folder containing a single 1MB JPG you will get around 64 chunks.</p>\n<p>After feeding the file contents through the chunker, we take the chunks and calculate the SHA256 hash of each one. We then arrange these hashes into a special data structure we developed that we call the Flat In-Order Merkle Tree.</p>\n<h3 id=\"flat-in-order-merkle-tree\">Flat In-Order Merkle Tree</h3>\n<pre><code>      <span class=\"hljs-number\">3</span>\n  <span class=\"hljs-number\">1</span>       <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">0</span>   <span class=\"hljs-number\">2</span>   <span class=\"hljs-number\">4</span>   <span class=\"hljs-number\">6</span>\n</code></pre><p>Want to go lower level? Check out <a href=\"https://github.com/datproject/docs/blob/master/docs/hyperdrive_spec.md#how-hypercore-works\">How Hypercore Works</a></p>\n<p>When two peers connect to each other and begin speaking the hyperdrive protocol they can efficiently determine if they have chunks the other one wants, and begin exchanging those chunks directly. Hyperdrive gives us the flexibility to have random access to any portion of a file while still verifying the other side isn&#39;t sending us bad data. We can also download different sections of files in parallel across all of the sources simultaneously, which increases overall download speed dramatically.</p>\n<h2 id=\"phase-4-data-archiving\">Phase 4: Data archiving</h2>\n<p>So now that you&#39;ve discovered, connected, and downloaded a copy of some data you can stick around for a while and serve up copies of the data to others who come along and want to download it.</p>\n<p>The first phase, source discovery, is actually an ongoing process. When you first search for data sources you only get the sources available at the time you did your search, so we make sure to perform discovery searches as often is practically possible to make sure new sources can be found and connected to.</p>\n<p>Every user of Dat is a source as long as they have 1 or more chunks of data. Just like with other decentralized file sharing protocols you will notice Dat may start uploading data before it finishes downloading.</p>\n<p>If the original source who shared the data goes offline it&#39;s OK, as long as other sources are available. As part of the mission as a not-for-profit we will be working with various institutions to ensure there are always sources available to accept new copies of data and stay online to serve those copies for important datasets such as scientific research data, open government data etc.</p>\n<p>Because Dat is built on a foundation of strong cryptographic data integrity and content addressable storage it gives us the possibility of implementing some really interesting version control techniques in the future. In that scenario archival data sources could choose to offer more disk space and archive every version of a Dat repository, whereas normal Dat users might only download and share one version that they happen to be interested in.</p>\n<h2 id=\"implementations\">Implementations</h2>\n<p>This covered a lot of ground. If you want to go deeper and see the implementations we are using in the <a href=\"https://github.com/datproject/dat\">Dat command-line tool</a>, go to the <a href=\"/ecosystem\">Dependencies</a> page</p>\n","faq":"<h1 id=\"faq\">FAQ</h1>\n<h2 id=\"is-dat-different-from-hyperdrive-\">Is Dat different from hyperdrive?</h2>\n<p><a href=\"http://github.com/mafintosh/hyperdrive\">Hyperdrive</a> is a file sharing network originally built for dat.</p>\n<p>Dat uses hyperdrive and a variety of other modules. Hyperdrive and Dat are compatible with each other but hyperdrive is able to make more lower-level decisions. Dat presents a user-friendly interface and ecosystem for scientists, researchers, and data analysts.</p>\n<h2 id=\"how-is-dat-different-than-ipfs-\">How is Dat different than IPFS?</h2>\n<h2 id=\"is-there-javascript-implementation-\">Is there JavaScript implementation?</h2>\n<p>Yes, find it on GitHub: <a href=\"http://github.com/joehand/dat-js\">dat-js</a>.</p>\n<h2 id=\"is-there-any-non-persistent-js-dat-implementation-\">Is there any non-persistent JS Dat implementation?</h2>\n<p>Not yet. Want to work on it? Start here to learn more: <a href=\"http://github.com/joehand/dat-js\">dat-js</a>.</p>\n<h2 id=\"is-there-an-online-dataset-registry-like-github-\">Is there an online dataset registry, like GitHub?</h2>\n<p>Yes, but currently under heavy construction. See <a href=\"http://github.com/datproject/dat.land\">dat.land</a></p>\n<h2 id=\"is-there-a-desktop-application-\">Is there a desktop application?</h2>\n<p>Yes, but currently under heavy construction. See <a href=\"http://github.com/juliangruber/dat-desktop\">dat-desktop</a></p>\n<h2 id=\"do-you-plan-to-have-python-or-r-or-other-third-party-language-integrations-\">Do you plan to have Python or R or other third-party language integrations?</h2>\n<p>Yes. We are currently developing the serialization format (like .zip archives) called <a href=\"/sleep\">SLEEP</a> so that third-party libraries can read data without reimplementing all of hyperdrive (which is node-only).</p>\n","browser":"<h1 id=\"browser-dat\">Browser Dat</h1>\n<p>Dat is written in JavaScript, so naturally, it can work entirely in the browser! The great part about this is that as more peers connect to each other in their client, the site assets will be shared between users rather hitting any sever.</p>\n<p>This approach is similar to that used in Feross&#39; <a href=\"http://webtorrent.io\">Web Torrent</a>. The difference is that Dat links can be rendered live and read dynamically, whereas BitTorrent links are static. In other words, the original owner can update a Dat and all peers will receive the updates automatically.</p>\n<p>OK, now for the goods:</p>\n<h2 id=\"hyperdrive\">Hyperdrive</h2>\n<p>For now, there isn&#39;t an easy dat implementation for the browser. We have a simpler interface for node at <a href=\"http://github.com/joehand/dat-js\">dat-js</a>.  </p>\n<p>If you want to get your hands dirty, here is the lower-level implementations to create a browser-based hyperdrive instance that will be compatible with dat.</p>\n<p>Hyperdrive will save the metadata (small) and the content (potentially large) separately. You can control where both of these are saved and how they are retrieved. These tweaks have huge impact on performance, stability, and user experience, so it&#39;s important to understand the tradeoffs.</p>\n<p>The first argument to <code>hyperdrive</code> will be the main database for all metadata and content. The <code>file</code> option can be supplied to specify how to read and write content data. If a <code>file</code> option is not supplied, the content will also be stored in the main database.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(&lt;YOUR DATABASE HERE&gt;, {file: &lt;CONTENT DATABASE HERE&gt;})\n</code></pre>\n<h3 id=\"the-most-basic-example\">The most basic example</h3>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> memdb = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'memdb'</span>)\n<span class=\"hljs-keyword\">var</span> swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive-archive-swarm'</span>)\n\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(memdb())\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive()\n\n<span class=\"hljs-comment\">// joins the webrtc swarm</span>\nswarm(archive)\n\n<span class=\"hljs-comment\">// this key can be used in another browser tab</span>\n<span class=\"hljs-built_in\">console</span>.log(archive.key)\n</code></pre>\n<p>That&#39;s it. Now you are serving a dat-compatible hyperdrive from the browser. In another browser tab, you can connect to the swarm and download the data by using the same code as above. Just make sure to reference the hyperdrive you created before by using <code>archive.key</code> as the first argument:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> drive = hyperdrive(memdb())\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(<span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">KEY</span> <span class=\"hljs-attr\">HERE</span>&gt;</span>)\n\n// joins the webrtc swarm\nswarm(archive)</span>\n</code></pre>\n<p>For the full hyperdrive API and more examples, see the full <a href=\"/hyperdrive\">hyperdrive documentation</a>.</p>\n<h2 id=\"patterns-for-browser-based-data-storage-and-transfer\">Patterns for browser-based data storage and transfer</h2>\n<p>There are a million different ways to store and retrieve data in the browser, and all have their pros and cons depending on the use case. We&#39;ve compiled a variety of examples here to try to make it as clear as possible.</p>\n<h3 id=\"in-memory-storage\">In-memory storage</h3>\n<p>When the user refreshes their browser, they will lose all previous keys and data. The user will no longer be able to write more data into the hyperdrive.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> memdb = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'memdb'</span>)\n\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(memdb())\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive()\n</code></pre>\n<h3 id=\"persistence-with-indexeddb\">Persistence with IndexedDB</h3>\n<p>When the user refreshes their browser, their keys will be stored and retrieved.</p>\n<p>The best module to use for this is <code>level-browserify</code>:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level-browserify'</span>)\n\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(level(<span class=\"hljs-string\">'./mydb'</span>))\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive()\n</code></pre>\n<p>This will store all of the hyperdrive metadata <em>as well as content</em> in the client&#39;s IndexedDB. This is pretty inefficient. You&#39;ll notice that with this method that <em>IndexedDB will start to become full and the hyperdrive database will stop working as usual</em>.</p>\n<h3 id=\"persistent-metadata-in-indexeddb-with-in-memory-file-content\">Persistent metadata in IndexedDB with in-memory file content</h3>\n<p>If you use level-browserify to store file content, you will quickly notice performance issues with large files. Writes after about 3.4GB will become blocked by the browser. You can avoid this by using in-memory storage for the file content.</p>\n<p>To do this, use <a href=\"https://github.com/mafintosh/random-access-file-reader\">random-access-file-reader</a> as the file writer and reader for the hyperdrive.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level-browserify'</span>)\n<span class=\"hljs-keyword\">var</span> ram = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-memory'</span>)\n\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(level(<span class=\"hljs-string\">'./mydb'</span>))\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  <span class=\"hljs-attr\">file</span>: ram\n})\n</code></pre>\n<p>This works well for most cases until you want to write a file to hyperdrive that doesn&#39;t fit in memory.</p>\n<h3 id=\"writing-large-files-from-the-filesystem-to-the-browser\">Writing large files from the filesystem to the browser</h3>\n<p>File writes are limited to the available memory on the machine. Files are buffered (read: copied) <em>into memory</em> while being written to the hyperdrive instance. This isn&#39;t ideal, but works as long as file sizes stay below system RAM limits.</p>\n<p>To fix this problem, you can use <a href=\"https://github.com/mafintosh/random-access-file-reader\">random-access-file-reader</a> to read the files directly from the filesystem instead of buffering them into memory.</p>\n<p>Here we will create a simple program that creates a file &#39;drag and drop&#39; element on <code>document.body.</code> When the user drags files onto the element, pointers to them will be added to the <code>files</code> object.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> drop = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'drag-drop'</span>)\n\n<span class=\"hljs-keyword\">var</span> files = {}\n\ndrop(<span class=\"hljs-built_in\">document</span>.body, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">files</span>) </span>{\n  files[files[<span class=\"hljs-number\">0</span>].name] = files[<span class=\"hljs-number\">0</span>]\n})\n</code></pre>\n<p>Okay, that&#39;s pretty easy. Now let&#39;s add the hyperdrive. Hyperdrive needs to know what the pointers are, so when a peer asks for the file, it can read from the filesystem rather from memory. In other words, we are telling the hyperdrive which files it should index.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> drop = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'drag-drop'</span>)\n<span class=\"hljs-keyword\">var</span> reader = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-file-reader'</span>)\n<span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> memdb = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'memdb'</span>)\n\n<span class=\"hljs-keyword\">var</span> files = {}\n\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(memdb())\n\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  <span class=\"hljs-attr\">file</span>: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-keyword\">return</span> reader(files[name])\n  }\n})\n\ndrop(<span class=\"hljs-built_in\">document</span>.body, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">files</span>) </span>{\n  files[files[<span class=\"hljs-number\">0</span>].name] = files[<span class=\"hljs-number\">0</span>]\n  <span class=\"hljs-comment\">// will index the file using hyperdrive without reading the entire file into ram</span>\n  archive.append(files[<span class=\"hljs-number\">0</span>].name)\n})\n</code></pre>\n<h2 id=\"unsatisfied-\">Unsatisfied?</h2>\n<p>If you still aren&#39;t satisfied, come over to our community channels and ask a question. It&#39;s probably a good one and we should cover it in the documentation. Thanks for trying it out, and PRs always welcome!</p>\n<p><a href=\"http://webchat.freenode.net/?channels=dat\"><img src=\"https://img.shields.io/badge/irc%20channel-%23dat%20on%20freenode-blue.svg\" alt=\"#dat IRC channel on freenode\"></a>\n<a href=\"https://gitter.im/datproject/discussions?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\"><img src=\"https://badges.gitter.im/Join%20Chat.svg\" alt=\"datproject/discussions\"></a></p>\n","diy-dat":"<h1 id=\"build-with-dat\">Build with Dat</h1>\n<p>In this guide, we will show how to develop applications with the Dat ecosystem. The Dat ecosystem is very modular making it easy to develop custom applications using Dat.</p>\n<p>For any Dat application, there are three essential modules you will start with: </p>\n<ol>\n<li><a href=\"https://npmjs.org/hyperdrive\">hyperdrive</a> for file synchronization and versioning</li>\n<li><a href=\"https://npmjs.org/hyperdrive-archive-swarm\">hyperdrive-archive-swarm</a> helps discover and connect to peers over local networks and the internet</li>\n<li>A <a href=\"https://npmjs.org/level\">LevelDB</a> compatible database for storing metadata.</li>\n</ol>\n<p>The <a href=\"https://npmjs.org/dat\">Dat CLI</a> module itself combines these modules and wraps them in a command-line API. These modules can be swapped out for a similarly compatible module, such as switching LevelDb for <a href=\"https://github.com/juliangruber/memdb\">MemDB</a> (which we do in the first example). More details on how these module work together are available in <a href=\"how-dat-works.md\">How Dat Works</a>.</p>\n<h2 id=\"getting-started\">Getting Started</h2>\n<p>You will need node and npm installed to build with Dat. <a href=\"https://github.com/datproject/dat/blob/master/CONTRIBUTING.md#development-workflow\">Read more</a> about our development work flow to learn how we manage our module dependencies during development.</p>\n<h2 id=\"module-1-download-a-file\">Module #1: Download a File</h2>\n<p>Our first module will download files from a Dat link entered by the user. View the code for this module on <a href=\"https://github.com/joehand/diy-dat-examples/tree/master/module-1\">Github</a>.</p>\n<pre><code class=\"lang-bash\">mkdir module-1 &amp;&amp; <span class=\"hljs-built_in\">cd</span> module-1\nnpm init\nnpm install --save hyperdrive memdb hyperdrive-archive-swarm\ntouch index.js\n</code></pre>\n<p>For this example, we will use <a href=\"https://github.com/juliangruber/memdb\">memdb</a> for our database (keeping the metadata in memory rather than on the file system). In your <code>index.js</code> file, require the main modules and set them up:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> memdb = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'memdb'</span>)\n<span class=\"hljs-keyword\">var</span> Hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> Swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive-archive-swarm'</span>)\n\n<span class=\"hljs-keyword\">var</span> link = process.argv[<span class=\"hljs-number\">2</span>] <span class=\"hljs-comment\">// user inputs the dat link</span>\n\n<span class=\"hljs-keyword\">var</span> db = memdb()\n<span class=\"hljs-keyword\">var</span> drive = Hyperdrive(db)\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(link)\n<span class=\"hljs-keyword\">var</span> swarm = Swarm(archive)\n</code></pre>\n<p>Notice, the user will input the link for the second argument The easiest way to get a file from a hyperdrive archive is to make a read stream. <code>archive.createFileReadStream</code> accepts the index number of filename for the first argument. To display the file, we can create a file stream and pipe it to <code>process.stdout</code>.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> stream = archive.createFileReadStream(<span class=\"hljs-number\">0</span>) <span class=\"hljs-comment\">// get the first file</span>\nstream.pipe(process.stdout)\n</code></pre>\n<p>Now, you can run the module! To download the first file from our docs Dat, run:</p>\n<pre><code><span class=\"hljs-keyword\">node</span> <span class=\"hljs-title\">index</span>.js <span class=\"hljs-number\">395</span>e3467bb5b2fa083ee8a4a17a706c5574b740b5e1be6efd65754d4ab7328c2\n</code></pre><p>You should see the first file in our docs repo.</p>\n<h4 id=\"module-1-bonus-display-any-file-in-the-dat\">Module #1 Bonus: Display any file in the Dat</h4>\n<p>With a few more lines of code, the user can enter a file to display from the Dat link.</p>\n<p>Challenge: create a module that will allow the user to input a Dat link and a filename: <code>node bonus.js &lt;dat-link&gt; &lt;filename&gt;</code>. The module will print out that file from the link, as we did above. To get a specific file you can change the file stream to use the filename instead of the index number:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> stream = archive.createFileReadStream(fileName)\n</code></pre>\n<p>Once you are finished, see if you can view this file by running:</p>\n<pre><code class=\"lang-bash\">node bonus.js 395e3467bb5b2fa083ee8a4a17a706c5574b740b5e1be6efd65754d4ab7328c2 cookbook/diy-dat.md\n</code></pre>\n<p><a href=\"https://github.com/joehand/diy-dat-examples/blob/master/module-1/bonus.js\">See how we coded it</a>. </p>\n<h2 id=\"module-2-download-all-files-to-computer\">Module #2: Download all files to computer</h2>\n<p>This module will build on the last module. Instead of displaying a single file, we will download all of the files from a Dat into a local directory. View the code for this module on <a href=\"https://github.com/joehand/diy-dat-examples/tree/master/module-2\">Github</a>.</p>\n<p>To download the files to the file system, instead of to a database, we will use the <code>file</code> option in <code>hyperdrive</code> and the <a href=\"http://npmjs.org/random-access-file\">random-access-file</a> module. We will also learn two new archive functions that make handling all the files a bit easier than the file stream in module #1. </p>\n<p>Setup will be the same as before (make sure you install random-access-file and stream-each this time): </p>\n<pre><code class=\"lang-bash\">mkdir module-2 &amp;&amp; <span class=\"hljs-built_in\">cd</span> module-2\nnpm init\nnpm install --save hyperdrive memdb hyperdrive-archive-swarm random-access-file stream-each\ntouch index.js\n</code></pre>\n<p>The first part of the module will look the same. We will add random-access-file (and <a href=\"http://npmjs.org/stream-each\">stream-each</a> to make things easier). The only difference is that we have to specify the <code>file</code> option when creating our archive:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> memdb = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'memdb'</span>)\n<span class=\"hljs-keyword\">var</span> Hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> Swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive-archive-swarm'</span>)\n<span class=\"hljs-keyword\">var</span> raf = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-file'</span>) <span class=\"hljs-comment\">// this is new!</span>\n<span class=\"hljs-keyword\">var</span> each = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'stream-each'</span>)\n\n<span class=\"hljs-keyword\">var</span> link = process.argv[<span class=\"hljs-number\">2</span>]\n\n<span class=\"hljs-keyword\">var</span> db = memdb()\n<span class=\"hljs-keyword\">var</span> drive = Hyperdrive(db)\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(link, {\n  <span class=\"hljs-attr\">file</span>: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-keyword\">return</span> raf(path.join(<span class=\"hljs-string\">'download'</span>, name)) <span class=\"hljs-comment\">// download into a \"download\" dir</span>\n  }\n})\n<span class=\"hljs-keyword\">var</span> swarm = Swarm(archive)\n</code></pre>\n<p>Now that we are setup, we can work with the archive. The <code>archive.download</code> function downloads the file content (to wherever you specified in the file option). To download all the files, we will need a list of files and then we will call download on each of them. <code>archive.list</code> will give us the list of the files. We use the stream-each module to make it easy to iterate over each item in the archive, then exit when the stream is finished.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> stream = archive.list({<span class=\"hljs-attr\">live</span>: <span class=\"hljs-literal\">false</span>}) <span class=\"hljs-comment\">// Use {live: false} for now to make the stream easier to handle.</span>\neach(stream, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">entry, next</span>) </span>{\n  archive.download(entry, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err</span>) </span>{\n    <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">console</span>.error(err)\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'downloaded'</span>, entry.name)\n    next()\n  })\n}, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n  process.exit(<span class=\"hljs-number\">0</span>)\n})\n</code></pre>\n<p>You should be able to run the module and see all our docs files in the <code>download</code> folder:</p>\n<pre><code class=\"lang-bash\">node index.js 395e3467bb5b2fa083ee8a4a17a706c5574b740b5e1be6efd65754d4ab7328c2\n</code></pre>\n<h2 id=\"module-3-sharing-a-file\">Module #3: Sharing a file</h2>\n<h2 id=\"module-4-sharing-a-directory-of-files\">Module #4: Sharing a directory of files</h2>\n","ecosystem":"<h1 id=\"dat-module-ecosystem\">Dat Module Ecosystem</h1>\n<p>We have built and contributed to a variety of modules that support our work on Dat as well as the larger data and code ecosystem. Feel free to go deeper and see the implementations we are using in the <a href=\"https://github.com/datproject/dat\">Dat command-line tool</a> and the <a href=\"https://github.com/joehand/dat-js\">Dat-js</a>, the javascript Dat module.</p>\n<p>Dat embraces the Unix philosophy: a modular design with composable parts. All of the pieces can be replaced with alternative implementations as long as they implement the abstract API.</p>\n<h2 id=\"public-interface-modules-\">Public Interface Modules:</h2>\n<ul>\n<li><a href=\"dat\">dat</a> - the command line interface for sharing and downloading files</li>\n<li><a href=\"dat.land\">dat.land</a> - repository for the <a href=\"https://dat.land\">dat.land</a> website, a public data registry and sharing</li>\n<li><a href=\"https://github.com/juliangruber/dat-desktop\">dat desktop</a> - dat desktop application for sharing and downloading files</li>\n</ul>\n<h2 id=\"file-and-block-component-modules-\">File and Block Component Modules:</h2>\n<ul>\n<li><a href=\"hyperdrive\">hyperdrive</a> - The file sharing network dat uses to distribute files and data. Read the technical <a href=\"hyperdrive-specification\">hyperdrive-specification</a> about how hyperdrive works.</li>\n<li><a href=\"hypercore\">hypercore</a> - exchange low-level binary blocks with many sources</li>\n<li><a href=\"https://www.npmjs.com/package/rabin\">rabin</a> - Rabin fingerprinter stream</li>\n<li><a href=\"https://www.npmjs.com/package/merkle-tree-stream\">merkle-tree-stream</a> - Used to construct Merkle trees from chunks</li>\n</ul>\n<h2 id=\"networking-peer-discovery-modules-\">Networking &amp; Peer Discovery Modules:</h2>\n<ul>\n<li><a href=\"https://www.npmjs.com/package/discovery-channel\">discovery-channel</a> - discover data sources</li>\n<li><a href=\"https://www.npmjs.com/package/discovery-swarm\">discovery-swarm</a> - discover and connect to sources</li>\n<li><a href=\"https://www.npmjs.com/package/bittorrent-dht\">bittorrent-dht</a> - use the Kademlia Mainline DHT to discover sources</li>\n<li><a href=\"https://www.npmjs.com/package/dns-discovery\">dns-discovery</a> - use DNS name servers and Multicast DNS to discover sources</li>\n<li><a href=\"https://www.npmjs.com/package/utp-native\">utp-native</a> - UTP protocol implementation</li>\n</ul>\n","sleep":"<h1 id=\"sleep-data-format\">SLEEP Data Format</h1>\n<h3 id=\"syncable-lightweight-event-emitting-persistence\">Syncable Lightweight Event Emitting Persistence</h3>\n<h3 id=\"version-2-0\">Version 2.0</h3>\n<p>SLEEP is a metadata format that allows a set of files to be accessed randomly, cryptographically verified, and dynamically updated. A SLEEP file contains content addressed file metadata in a representation specifically designed to allow partial streaming access to individual chunks of data. SLEEP files can be shared as a single downloadable file for easy distribution and we also specify a way to expose SLEEP over REST.</p>\n<p>The SLEEP format can be used in a similar way to how MD5 checksums are used over HTTP today, to verify the integrity of data downloads. Whereas MD5 or SHA are usually checksums of the whole data set, meaning consumers have to download the entire all available data before they are able to verify the integrity of any of it, SLEEP allows a set of data to be split in to many small pieces, each one getting it&#39;s own cryptographically secure checksum. This allows consumers to download subsets metadata and data, in whatever order they prefer, but allowing them to verify the integrity of each piece of data as it is accessed. It also includes cryptographic signatures allowing users to verify that data they received was created using a holder of a specific private key.</p>\n<h2 id=\"registers\">Registers</h2>\n<p>SLEEP is designed around the concept of a register, an append only list that you can trust. The contents of a register are cryptographically fingerprinted and an aggregate checksum can be used to verify the contents of the register have not been tampered with. There are various ways to calculate these aggregate checksums but the data in a register is a binary append only feed, e.g. an list of buffers that can only be updated by placing new buffers at the end of the list.</p>\n<p>SLEEP also provides an index that allows each piece of data in a register to be accessed randomly. In order to look up a specific piece of data in the register, you only need a small subset of the metadata in order to find it, making SLEEP suitable for live streaming or sparse download use cases.</p>\n<p>The register index is a Merkle tree where the leaf nodes are the hashes of the buffers in the register, and the rest of the nodes in the tree are derived Merkle hashes. A Merkle tree is defined as a tree where leaf nodes are a hash of some piece of data, and the rest of the nodes are the result of a hash of the concatenation of that nodes children.</p>\n<p>So, given a register with four values:</p>\n<pre><code><span class=\"hljs-bullet\">1. </span>a\n<span class=\"hljs-bullet\">2. </span>b\n<span class=\"hljs-bullet\">3. </span>c\n<span class=\"hljs-bullet\">4. </span>d\n</code></pre><p>To construct the register itself you concatenate all buffers, in this case resulting in &#39;abcd&#39;.</p>\n<p>The register index is constructed by creating a Merkle tree where the leaf nodes are the hash of our four values, and the rest of the nodes are the hash of the nodes two children hashes concatenated together.</p>\n<pre><code>hash(<span class=\"hljs-name\">a</span>)\n      &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">a</span>) + hash(<span class=\"hljs-name\">b</span>))\nhash(<span class=\"hljs-name\">b</span>)\n              &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">a</span>) + hash(<span class=\"hljs-name\">b</span>)) + hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">c</span>) + hash(<span class=\"hljs-name\">d</span>)))\nhash(<span class=\"hljs-name\">c</span>)\n      &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">c</span>) + hash(<span class=\"hljs-name\">d</span>))\nhash(<span class=\"hljs-name\">d</span>)\n</code></pre><p>To be able to refer to a specific node in the tree we use an in-order node traversal to assign integers to the nodes:</p>\n<pre><code><span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>In-order node numbering has the property with our trees that leaf nodes are always even and non-leaf nodes are always odd. This can be used as a quick way to identify whether a node is a leaf or not.</p>\n<p>Every serialized node in the tree is one of two fixed widths, leaf nodes are all the same size and non-leaf nodes are the same size. When serializing the tree you simply write the nodes in order and concatenate them. Then to access a node by its in-order position you simply multiply the node length by the position to get the byte offset.</p>\n<p>All leaf nodes contain these two pieces of information:</p>\n<ul>\n<li>The sha256 hash of the data described by this node</li>\n<li>The absolute byte offset to the end of the region of data described by the node</li>\n</ul>\n<p>All non-leaf nodes contain these three pieces of information:</p>\n<ul>\n<li>The sha256 hash of the concatenation of the two children hashes</li>\n<li>The cryptographic signature of the hash</li>\n<li>The span of bytes that the the nodes children cover</li>\n</ul>\n<p>When initializing a register an asymmetric Ed25519 keypair is derived. The private key is never shared. The public key is used as the URL for the register. When signing hashes in the tree the public key is used to generate an EdDSA signature. For the example register above, &#39;abcd&#39;, the register index (in JSON) would be:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> keys = {\n  <span class=\"hljs-attr\">public</span>: <span class=\"hljs-string\">'cc0cf6eeb82ca946ca60265ce0863fb2b3e3075ae25cba14d162ef20e3f9f223'</span>,\n  <span class=\"hljs-attr\">private</span>: <span class=\"hljs-string\">'87399f90815db81e687efe4fd9fc60af336f4d9ae560fda106f94cb7a92a8804cc0cf6eeb82ca946ca60265ce0863fb2b3e3075ae25cba14d162ef20e3f9f223'</span>\n}\n\n<span class=\"hljs-keyword\">var</span> index = {\n  <span class=\"hljs-comment\">// sha256 of children[0].hash + children[1].hash</span>\n  hash: <span class=\"hljs-string\">'0440c655d63fec5c02cffd5d9b42d146aca03b255102b9b44b51c6a919b31351'</span>,\n  <span class=\"hljs-attr\">signature</span>: <span class=\"hljs-string\">'1713dfbaf4a7f288003394b72ec486aa4fa1a837aa0b08662b3a14b63381b84c2e6965e2638fb5375ae2e92b47c2ab8718ec1914778518fcb3c0563eb2c09604'</span>,\n  <span class=\"hljs-attr\">span</span>: <span class=\"hljs-number\">4</span>,\n  <span class=\"hljs-attr\">children</span>: [\n    {\n      <span class=\"hljs-comment\">// echo -n \"$(echo -n \"a\" | shasum -a 256)$(echo -n \"b\" | shasum -a 256)\" | shasum -a 256</span>\n      hash: <span class=\"hljs-string\">'9ad4d5608a7a40db60c35f255fad821b762a82de168b4f4ed477d5d899b11796'</span>,\n      <span class=\"hljs-attr\">signature</span>: <span class=\"hljs-string\">'2714b99e305ce46aa6d24eb2888cf0cbde33ad4a8bcd08705b59882837bf1e482f8dcab2ae94c2359914b1fe92831bfc73af99f1c6b1f5eba47efc4efa32de0d'</span>,\n      <span class=\"hljs-attr\">span</span>: <span class=\"hljs-number\">2</span>,\n      <span class=\"hljs-attr\">children</span>: [\n        {\n          <span class=\"hljs-comment\">// echo -n \"a\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb'</span>,\n          <span class=\"hljs-attr\">endByte</span>: <span class=\"hljs-number\">1</span>\n        },\n        {\n          <span class=\"hljs-comment\">// echo -n \"b\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'3e23e8160039594a33894f6564e1b1348bbd7a0088d42c4acb73eeaed59c009d'</span>,\n          <span class=\"hljs-attr\">endByte</span>: <span class=\"hljs-number\">2</span>\n        }\n      ]\n    },\n    {\n      <span class=\"hljs-comment\">// echo -n \"$(echo -n \"c\" | shasum -a 256)$(echo -n \"d\" | shasum -a 256)\" | shasum -a 256</span>\n      hash: <span class=\"hljs-string\">'09114d1a8a78b5d091e492c524ad7f8e941f403db0a6d3d52d36f17b9a86ce1c'</span>,\n      <span class=\"hljs-attr\">signature</span>: <span class=\"hljs-string\">'6ac5e25206f69f22612e9b58c14f9ae6738233a57ab7f6e10c1384c4e074f6c8c606edbd95a9c099a0120947866079e3d13ef66dd7d5ed1756a89a5e9032a20d'</span>,\n      <span class=\"hljs-attr\">span</span>: <span class=\"hljs-number\">2</span>,\n      <span class=\"hljs-attr\">children</span>: [\n        {\n          <span class=\"hljs-comment\">// echo -n \"c\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'2e7d2c03a9507ae265ecf5b5356885a53393a2029d241394997265a1a25aefc6'</span>,\n          <span class=\"hljs-attr\">endByte</span>: <span class=\"hljs-number\">3</span>\n        },\n        {\n          <span class=\"hljs-comment\">// echo -n \"d\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'18ac3e7343f016890c510e93f935261169d9e3f565436429830faf0934f4f8e4'</span>,\n          <span class=\"hljs-attr\">endByte</span>: <span class=\"hljs-number\">4</span>\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n<p>The above representation of the tree is in JSON. However due to the properties of the in-order node indexes we can represent the same data in a flat index while still allowing traversals.</p>\n<h1 id=\"file-format\">File format</h1>\n<p>SLEEP files should be named <code>sleep.dat</code> and have the following format:</p>\n<pre><code>&lt;<span class=\"hljs-keyword\">Header</span>&gt;&lt;Register Index<span class=\"hljs-params\">...</span>&gt;&lt;Register <span class=\"hljs-built_in\">Data</span><span class=\"hljs-params\">...</span>&gt;\n</code></pre><p>The format is a header followed by the register index. Order of the index is determined by an in-order node traversal. After the register index, the actual register entry data follows. The header length is variable width, prefixed with a varint. The Register Index is composed of fixed width metadata entries. The Register Data is composed of concatenated non-fixed width data pieces.</p>\n<h3 id=\"header-format\">Header format</h3>\n<pre><code><span class=\"hljs-section\">&lt;varint header-length&gt;</span><span class=\"hljs-section\">&lt;header protobuf&gt;</span>\n</code></pre><p>The header protobuf has this schema:</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Header</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> datLink = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> entryCount = <span class=\"hljs-number\">2</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bool</span> isSigned = <span class=\"hljs-number\">3</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">string</span> hashType = <span class=\"hljs-number\">4</span> [default = <span class=\"hljs-string\">\"sha256\"</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint32</span> hashLength = <span class=\"hljs-number\">5</span> [default = <span class=\"hljs-number\">32</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">string</span> signatureType = <span class=\"hljs-number\">6</span> [default = <span class=\"hljs-string\">\"ed25519\"</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint32</span> signatureLength = <span class=\"hljs-number\">7</span> [default = <span class=\"hljs-number\">64</span>];\n}\n</code></pre>\n<h3 id=\"register-index-format\">Register Index format</h3>\n<p>For non-signed even (leaf) nodes:</p>\n<pre><code><span class=\"hljs-section\">&lt;8-byte-span-length&gt;</span><span class=\"hljs-section\">&lt;data-hash&gt;</span>\n</code></pre><p>The 8-byte-span-length is an unsigned big endian 64 bit integer that should be number of cumulative bytes encompassed by all of the leaf nodes underneath the current node.</p>\n<p>For signed even (leaf) nodes:</p>\n<pre><code><span class=\"hljs-section\">&lt;8-byte-span-length&gt;</span><span class=\"hljs-section\">&lt;data-hash-signature&gt;</span><span class=\"hljs-section\">&lt;data-hash&gt;</span>\n</code></pre><p>For odd (non-leaf) nodes:</p>\n<pre><code>&lt;<span class=\"hljs-number\">8</span>-<span class=\"hljs-keyword\">byte</span>-<span class=\"hljs-keyword\">end</span>-<span class=\"hljs-built_in\">offset</span>&gt;&lt;data-hash&gt;\n</code></pre><p>The 8-byte-end-offset is an unsigned big endian 64 bit integer that should be the absolute position in the file for the <strong>end</strong> of the piece data described by this node.</p>\n<h3 id=\"register-data\">Register Data</h3>\n<p>The last section of the file is the actual data pieces, unmodified and concatenated together in sequential order.</p>\n<p>For the example tree above, the Register Data section would simply be <code>abcd</code>.</p>\n<h2 id=\"example\">Example</h2>\n<p>Given a tree like this you might want to look up in a <code>meta.dat</code> file the metadata for a specific node:</p>\n<pre><code><span class=\"hljs-number\">0</span>  \n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span> \n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span> \n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>If you wanted to look up the metadata for 3, you could read the third (or any!) entry from sleep.dat:</p>\n<p>First you have to read the varint at the beginning of the file so you know how big the header is:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> varint = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'varint'</span>) <span class=\"hljs-comment\">// https://github.com/chrisdickinson/varint</span>\n<span class=\"hljs-keyword\">var</span> headerLength = varint.decode(firstChunkOfFile)\n</code></pre>\n<p>Now you can read the header from the file</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> headerOffset = varint.encodingLength(headerLength)\n<span class=\"hljs-keyword\">var</span> headerEndOffset = headerOffset + headerLength\n<span class=\"hljs-keyword\">var</span> headerBytes = firstChunkOfFile.slice(headerOffset, headerEndOffset)\n</code></pre>\n<p>To decode the header use the protobuf schema. We can use the <a href=\"https://github.com/mafintosh/protocol-buffers\">protocol-buffers</a> module to do that.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> messages = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'protocol-buffers'</span>)(fs.readFileSync(<span class=\"hljs-string\">'meta.dat.proto'</span>))\n<span class=\"hljs-keyword\">var</span> header = messages.Header.decode(headerBytes)\n</code></pre>\n<p>Now we have all the configuration required to calculate an entry offset.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> entryNumber = <span class=\"hljs-number\">42</span>\n<span class=\"hljs-keyword\">var</span> entryOffset = headerEndOffset + entryNumber * (<span class=\"hljs-number\">8</span> + header.hashLength)\n</code></pre>\n<p>If you have a signed feed, you have to take into account the extra space required for the signatures in the even nodes.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> entryOffset = headerLength + entryNumber * (<span class=\"hljs-number\">8</span> + header.hashLength)\n                  + <span class=\"hljs-built_in\">Math</span>.floor(entryNumber / <span class=\"hljs-number\">2</span>) * header.signatureLength\n</code></pre>\n","hyperdrive":"<h1 id=\"hyperdrive\">hyperdrive</h1>\n<p>A file sharing network based on <a href=\"https://github.com/maxogden/rabin\">rabin</a> file chunking and <a href=\"https://github.com/mafintosh/hypercore\">append only feeds of data verified by merkle trees</a>.</p>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> hyperdrive\n</code></pre><p><a href=\"http://travis-ci.org/mafintosh/hyperdrive\"><img src=\"http://img.shields.io/travis/mafintosh/hyperdrive.svg?style=flat\" alt=\"build status\"></a></p>\n<p>If you are interested in learning how hyperdrive works on a technical level a specification is available in the <a href=\"https://github.com/datproject/docs/blob/master/docs/hyperdrive_spec.md\">Dat docs repo</a></p>\n<h2 id=\"usage\">Usage</h2>\n<p>First create a new feed and share it</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level'</span>)\n<span class=\"hljs-keyword\">var</span> swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'discovery-swarm'</span>)()\n\n<span class=\"hljs-keyword\">var</span> db = level(<span class=\"hljs-string\">'./hyperdrive.db'</span>)\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(db)\n\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive()\n<span class=\"hljs-keyword\">var</span> ws = archive.createFileWriteStream(<span class=\"hljs-string\">'hello.txt'</span>) <span class=\"hljs-comment\">// add hello.txt</span>\n\nws.write(<span class=\"hljs-string\">'hello'</span>)\nws.write(<span class=\"hljs-string\">'world'</span>)\nws.end()\n\n<span class=\"hljs-keyword\">var</span> link = archive.key.toString(<span class=\"hljs-string\">'hex'</span>)\n<span class=\"hljs-built_in\">console</span>.log(link, <span class=\"hljs-string\">'&lt;-- this is your hyperdrive link'</span>)\n\n<span class=\"hljs-comment\">// the archive is now ready for sharing.</span>\n<span class=\"hljs-comment\">// we can use swarm to replicate it to other peers</span>\nswarm.listen()\nswarm.join(<span class=\"hljs-keyword\">new</span> Buffer(link, <span class=\"hljs-string\">'hex'</span>))\nswarm.on(<span class=\"hljs-string\">'connection'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">connection</span>) </span>{\n  connection.pipe(archive.replicate()).pipe(connection)\n})\n</code></pre>\n<p>Then we can access the content from another process with the following code</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'discovery-swarm'</span>)()\n<span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level'</span>)\n\n<span class=\"hljs-keyword\">var</span> db = level(<span class=\"hljs-string\">'./another-hyperdrive.db'</span>)\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(db)\n\n<span class=\"hljs-keyword\">var</span> link = <span class=\"hljs-keyword\">new</span> Buffer(<span class=\"hljs-string\">'your-hyperdrive-link-from-the-above-example'</span>, <span class=\"hljs-string\">'hex'</span>)\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(link)\n\nswarm.listen()\nswarm.join(link)\nswarm.on(<span class=\"hljs-string\">'connection'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">connection</span>) </span>{\n  connection.pipe(archive.replicate()).pipe(connection)\n  archive.get(<span class=\"hljs-number\">0</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err, entry</span>) </span>{ <span class=\"hljs-comment\">// get the first file entry</span>\n    <span class=\"hljs-built_in\">console</span>.log(entry) <span class=\"hljs-comment\">// prints {name: 'hello.txt', ...}</span>\n    <span class=\"hljs-keyword\">var</span> stream = archive.createFileReadStream(entry)\n    stream.on(<span class=\"hljs-string\">'data'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">data</span>) </span>{\n      <span class=\"hljs-built_in\">console</span>.log(data) <span class=\"hljs-comment\">// &lt;-- file data</span>\n    })\n    stream.on(<span class=\"hljs-string\">'end'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n      <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'no more data'</span>)\n    })\n  })\n})\n</code></pre>\n<p>If you want to write/read files to the file system provide a storage driver as the file option</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> raf = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-file'</span>) <span class=\"hljs-comment\">// a storage driver that writes to the file system</span>\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  <span class=\"hljs-attr\">file</span>: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-keyword\">return</span> raf(<span class=\"hljs-string\">'my-download-folder/'</span> + name)\n  }\n})\n</code></pre>\n<h2 id=\"api\">API</h2>\n<h4 id=\"-var-drive-hyperdrive-db-\"><code>var drive = hyperdrive(db)</code></h4>\n<p>Create a new hyperdrive instance. db should be a <a href=\"https://github.com/level/levelup\">levelup</a> instance.</p>\n<h4 id=\"-var-archive-drive-createarchive-key-options-\"><code>var archive = drive.createArchive([key], [options])</code></h4>\n<p>Creates an archive instance. If you want to download/upload an existing archive provide the archive key\nas the first argument. Options include</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">live</span>: <span class=\"hljs-literal\">false</span>, <span class=\"hljs-comment\">// set this to share the archive without finalizing it</span>\n  sparse: <span class=\"hljs-literal\">false</span>, <span class=\"hljs-comment\">// set this to only download the pieces of the feed you are requesting / prioritizing</span>\n  file: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-comment\">// set this to determine how file data is stored.</span>\n    <span class=\"hljs-comment\">// the storage instance should implement the hypercore storage api</span>\n    <span class=\"hljs-comment\">// https://github.com/mafintosh/hypercore#storage-api</span>\n    <span class=\"hljs-keyword\">return</span> someStorageInstance\n  }\n}\n</code></pre>\n<p>If you do not provide the file option all file data is stored in the leveldb.</p>\n<h4 id=\"-archive-key-\"><code>archive.key</code></h4>\n<p>A buffer that verifies the archive content. In live mode this is a 32 byte public key.\nOtherwise it is a 32 byte hash.</p>\n<h4 id=\"-archive-live-\"><code>archive.live</code></h4>\n<p>Boolean whether archive is live. <code>true</code> by default. Note that its only populated after archive.open(cb) has been fired.</p>\n<h4 id=\"-archive-append-entry-callback-\"><code>archive.append(entry, callback)</code></h4>\n<p>Append an entry to the archive. Only possible if this is an live archive you originally created\nor an unfinalized archive.</p>\n<p>If you set the file option in the archive constructor you can use this method to append an already\nexisting file to the archive.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  <span class=\"hljs-attr\">file</span>: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'returning storage for'</span>, name)\n    <span class=\"hljs-keyword\">return</span> raf(name)\n  }\n})\n\narchive.append(<span class=\"hljs-string\">'hello.txt'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'hello.txt was read and appended'</span>)\n})\n</code></pre>\n<h4 id=\"-archive-finalize-callback-\"><code>archive.finalize([callback])</code></h4>\n<p>Finalize the archive. You need to do this before sharing it if the archive is not live (it is live per default).</p>\n<h4 id=\"-archive-get-index-callback-\"><code>archive.get(index, callback)</code></h4>\n<p>Reads an entry from the archive.</p>\n<h4 id=\"-archive-download-index-callback-\"><code>archive.download(index, callback)</code></h4>\n<p>Fully downloads a file / entry from the archive and calls the callback afterwards.</p>\n<h4 id=\"-archive-close-callback-\"><code>archive.close([callback])</code></h4>\n<p>Closes and releases all resources used by the archive. Call this when you are done using it.</p>\n<h4 id=\"-archive-on-download-data-\"><code>archive.on(&#39;download&#39;, data)</code></h4>\n<p>Emitted every time a piece of data is downloaded</p>\n<h4 id=\"-archive-on-upload-data-\"><code>archive.on(&#39;upload&#39;, data)</code></h4>\n<p>Emitted every time a piece of data is uploaded</p>\n<h4 id=\"-var-rs-archive-list-opts-cb-\"><code>var rs = archive.list(opts={}, cb)</code></h4>\n<p>Returns a readable stream of all entries in the archive.</p>\n<ul>\n<li><code>opts.offset</code> - start streaming from this offset (default: 0)</li>\n<li><code>opts.live</code> - keep the stream open as new updates arrive (default: false)</li>\n</ul>\n<p>You can collect the results of the stream with <code>cb(err, entries)</code>.</p>\n<h4 id=\"-var-rs-archive-createfilereadstream-entry-options-\"><code>var rs = archive.createFileReadStream(entry, [options])</code></h4>\n<p>Returns a readable stream of the file content of an file in the archive.</p>\n<p>Options include:</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">start</span>: startOffset, <span class=\"hljs-comment\">// defaults to 0</span>\n  end: endOffset <span class=\"hljs-comment\">// defaults to file.length</span>\n}\n</code></pre>\n<h4 id=\"-var-ws-archive-createfilewritestream-entry-\"><code>var ws = archive.createFileWriteStream(entry)</code></h4>\n<p>Returns a writable stream that writes a new file to the archive. Only possible if the archive is live and you own it\nor if the archive is not finalized.</p>\n<h4 id=\"-var-cursor-archive-createbytecursor-entry-options-\"><code>var cursor = archive.createByteCursor(entry, [options])</code></h4>\n<p>Creates a cursor that can seek and traverse parts of the file.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> cursor = archive.createByteCursor(<span class=\"hljs-string\">'hello.txt'</span>)\n\n<span class=\"hljs-comment\">// seek to byte offset 10000 and read the rest.</span>\ncursor.seek(<span class=\"hljs-number\">10000</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err</span>) </span>{\n  <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">throw</span> err\n  cursor.next(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> <span class=\"hljs-title\">loop</span> (<span class=\"hljs-params\">err, data</span>) </span>{\n    <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">throw</span> err\n    <span class=\"hljs-keyword\">if</span> (!data) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'no more data'</span>)\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'cursor.position is '</span> + cursor.position)\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'read'</span>, data.length, <span class=\"hljs-string\">'bytes'</span>)\n    cursor.next(loop)\n  })\n})\n</code></pre>\n<p>Options include</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">start</span>: startOffset, <span class=\"hljs-comment\">// defaults to 0</span>\n  end: endOffset <span class=\"hljs-comment\">// defaults to file.length</span>\n}\n</code></pre>\n<h4 id=\"-var-stream-archive-replicate-\"><code>var stream = archive.replicate()</code></h4>\n<p>Pipe this stream together with another peer that is interested in the same archive to replicate the content.</p>\n<h2 id=\"license\">License</h2>\n<p>MIT</p>\n","hypercore":"<h1 id=\"hypercore\">hypercore</h1>\n<p>Hypercore is a protocol and p2p network for distributing and replicating feeds of binary data. It is the low level component that <a href=\"https://github.com/mafintosh/hyperdrive\">Hyperdrive</a> is built on top of.</p>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> hypercore\n</code></pre><p><a href=\"http://travis-ci.org/mafintosh/hypercore\"><img src=\"http://img.shields.io/travis/mafintosh/hypercore.svg?style=flat\" alt=\"build status\"></a></p>\n<p>It runs both in the node and in the browser using <a href=\"https://github.com/substack/node-browserify\">browserify</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hypercore = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hypercore'</span>)\n<span class=\"hljs-keyword\">var</span> net = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'net'</span>)\n\n<span class=\"hljs-keyword\">var</span> core = hypercore(db) <span class=\"hljs-comment\">// db is a leveldb instance</span>\n<span class=\"hljs-keyword\">var</span> feed = core.createFeed()\n\nfeed.append([<span class=\"hljs-string\">'hello'</span>, <span class=\"hljs-string\">'world'</span>], <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'appended two blocks'</span>)\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'key is'</span>, feed.key.toString(<span class=\"hljs-string\">'hex'</span>))\n})\n\nfeed.on(<span class=\"hljs-string\">'upload'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">block, data</span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'uploaded block'</span>, block, data)\n})\n\n<span class=\"hljs-keyword\">var</span> server = net.createServer(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">socket</span>) </span>{\n  socket.pipe(feed.replicate()).pipe(socket)\n})\n\nserver.listen(<span class=\"hljs-number\">10000</span>)\n</code></pre>\n<p>In another process</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> core = hypercore(anotherDb)\n<span class=\"hljs-keyword\">var</span> feed = core.createFeed(<span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">key-printed-out-above</span>&gt;</span>)\nvar socket = net.connect(10000)\n\nsocket.pipe(feed.replicate()).pipe(socket)\n\nfeed.on('download', function (block, data) {\n  console.log('downloaded block', block, data)\n})</span>\n</code></pre>\n<h2 id=\"api\">API</h2>\n<h4 id=\"-var-core-hypercore-db-\"><code>var core = hypercore(db)</code></h4>\n<p>Create a new hypercore instance. <code>db</code> should be a leveldb instance.</p>\n<h4 id=\"-var-feed-core-createfeed-key-options-\"><code>var feed = core.createFeed([key], [options])</code></h4>\n<p>Create a new feed. A feed stores a list of append-only data (buffers). A feed has a <code>.key</code> property that you can pass in to <code>createFeed</code> if you want to retrieve an old feed. Per default all feeds are appendable (live).</p>\n<p>Options include:</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">live</span>: <span class=\"hljs-literal\">true</span>,\n  <span class=\"hljs-attr\">storage</span>: externalStorage,\n  <span class=\"hljs-attr\">sparse</span>: <span class=\"hljs-literal\">false</span>\n}\n</code></pre>\n<p>Set <code>sparse</code> to <code>true</code> if you only want to download the pieces of the feed you are requesting / prioritizing. Otherwise the entire feed will be downloaded if nothing else is prioritized.</p>\n<p>If you want to create a static feed, one you cannot reappend data to, pass the <code>{live: false}</code> option.\nThe storage option allows you to store data outside of leveldb. This is very useful if you use hypercore to distribute files.</p>\n<p>See the <a href=\"#storage-api\">Storage API</a> section for more info</p>\n<h4 id=\"-var-stream-core-replicate-opts-\"><code>var stream = core.replicate(opts)</code></h4>\n<p>Create a generic replication stream. Use the <code>feed.replicate(stream)</code> API described below to replicate specific feeds of data.\nOptions include:</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">upload</span>: <span class=\"hljs-literal\">true</span>, <span class=\"hljs-comment\">// upload data to other peer</span>\n  download: <span class=\"hljs-literal\">true</span> <span class=\"hljs-comment\">// download data from other peer</span>\n}\n</code></pre>\n<h4 id=\"-var-stream-core-list-options-callback-\"><code>var stream = core.list([options], [callback])</code></h4>\n<p>List all feed keys in the database. Optionally you can pass a callback to buffer them into an array. Options include:</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">values</span>: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// set this to get feed attributes, not just feed keys</span>\n}\n</code></pre>\n<h2 id=\"-feed-api-\"><code>Feed API</code></h2>\n<p>As mentioned above a feed stores a list of data for you that you can replicate to other peers. It has the following API</p>\n<h4 id=\"-feed-key-\"><code>feed.key</code></h4>\n<p>The key of this feed. A 32 byte buffer. Other peers need this key to start replicating the feed.</p>\n<h4 id=\"-feed-discoverykey-\"><code>feed.discoveryKey</code></h4>\n<p>A 32 byte buffer containing a discovery key of the feed. The discovery key is sha-256 hmac of the string <code>hypercore</code> using the feed key as the password.\nYou can use the discovery key to find other peers sharing this feed without disclosing your feed key to a third party.</p>\n<h4 id=\"-feed-blocks-\"><code>feed.blocks</code></h4>\n<p>The total number of known data blocks in the feed.</p>\n<h4 id=\"-feed-bytes-\"><code>feed.bytes</code></h4>\n<p>The total byte size of known data blocks in the feed.</p>\n<h4 id=\"-feed-open-cb-\"><code>feed.open(cb)</code></h4>\n<p>Call this method to ensure that a feed is opened. You do not need to call this but the <code>.blocks</code> property will not be populated until the feed has been opened.</p>\n<h4 id=\"-feed-append-data-callback-\"><code>feed.append(data, callback)</code></h4>\n<p>Append a block of data to the feed. If you want to append more than one block you can pass in an array.</p>\n<h4 id=\"-feed-get-index-callback-\"><code>feed.get(index, callback)</code></h4>\n<p>Retrieve a block of data from the feed.</p>\n<h4 id=\"-feed-prioritize-range-callback-\"><code>feed.prioritize(range, [callback])</code></h4>\n<p>Prioritize a range of blocks to download. Will call the callback when done.\nRange should look like this</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">start</span>: startBlock,\n  <span class=\"hljs-attr\">end</span>: optionalEndBlock,\n  <span class=\"hljs-attr\">priority</span>: <span class=\"hljs-number\">2</span> <span class=\"hljs-comment\">// a priority level spanning [0-5]</span>\n  linear: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// download the range linearly</span>\n}\n</code></pre>\n<h4 id=\"-feed-unprioritize-range-\"><code>feed.unprioritize(range)</code></h4>\n<p>Unprioritize a range.</p>\n<h4 id=\"-feed-seek-byteoffset-callback-\"><code>feed.seek(byteOffset, callback)</code></h4>\n<p>Find the block of data containing the byte offset. Calls the callback with <code>(err, index, offset)</code> where <code>index</code> is the block index and <code>offset</code> is the the relative byte offset in the block returned by <code>.get(index)</code>.</p>\n<h4 id=\"-feed-finalize-callback-\"><code>feed.finalize(callback)</code></h4>\n<p>If you are not using a live feed you need to call this method to finalize the feed once you are ready to share it.\nFinalizing will set the <code>.key</code> property and allow other peers to get your data.</p>\n<h4 id=\"-var-stream-feed-createwritestream-options-\"><code>var stream = feed.createWriteStream([options])</code></h4>\n<p>Create a writable stream that appends to the feed. If the feed is a static feed, it will be finalized when you end the stream.</p>\n<h4 id=\"-var-stream-feed-createreadstream-options-\"><code>var stream = feed.createReadStream([options])</code></h4>\n<p>Create a readable stream that reads from the feed. Options include:</p>\n<pre><code class=\"lang-js\">{\n  <span class=\"hljs-attr\">start</span>: startIndex, <span class=\"hljs-comment\">// read from this index</span>\n  end: endIndex, <span class=\"hljs-comment\">// read until this index</span>\n  live: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// set this to keep the read stream open</span>\n}\n</code></pre>\n<h4 id=\"-var-stream-feed-replicate-options-\"><code>var stream = feed.replicate([options])</code></h4>\n<p>Get a replication stream for this feed. Pipe this to another peer to start replicating this feed with another peer.\nIf you create multiple replication streams to multiple peers you&#39;ll upload/download data to all of them (meaning the load will spread out).</p>\n<p>Per default the replication stream encrypts all messages sent using the feed key and an incrementing nonce. This helps ensures that the remote peer also the feed key and makes it harder for a man-in-the-middle to sniff the data you are sending.</p>\n<p>Set <code>{private: false}</code> to disable this.</p>\n<p>Hypercore uses a simple multiplexed protocol that allows one replication stream to be used for multiple feeds at once.\nIf you want to join another replication stream simply pass it as the stream option</p>\n<pre><code class=\"lang-js\">feed.replicate({<span class=\"hljs-attr\">stream</span>: anotherReplicationStream})\n</code></pre>\n<p>As a shorthand you can also do <code>feed.replicate(stream)</code>.</p>\n<h4 id=\"-stream-on-open-discoverykey-\"><code>stream.on(&#39;open&#39;, discoveryKey)</code></h4>\n<p>Emitted when a remote feed joins the replication stream and you haven&#39;t. You can use this as a signal to join the stream yourself if you want to.</p>\n<h4 id=\"-feed-on-download-block-data-\"><code>feed.on(&#39;download&#39;, block, data)</code></h4>\n<p>Emitted when a data block has been downloaded</p>\n<h4 id=\"-feed-on-download-finished-\"><code>feed.on(&#39;download-finished&#39;)</code></h4>\n<p>Emitted when all available data has been downloaded.\nWill re-fire when a live feed is updated and you download all the new blocks.</p>\n<h4 id=\"-feed-on-upload-block-data-\"><code>feed.on(&#39;upload&#39;, block, data)</code></h4>\n<p>Emitted when a data block has been uploaded</p>\n<h2 id=\"storage-api\">Storage API</h2>\n<p>If you want to use external storage to store the hypercore data (metadata will still be stored in the leveldb) you need to implement the following api and provide that as the <code>storage</code> option when creating a feed.</p>\n<p>Some node modules that implement this interface are</p>\n<ul>\n<li><a href=\"https://github.com/mafintosh/random-access-file\">random-access-file</a> Writes data to a file.</li>\n<li><a href=\"https://github.com/mafintosh/random-access-memory\">random-access-memory</a> Writes data to memory.</li>\n</ul>\n<h4 id=\"-storage-open-cb-\"><code>storage.open(cb)</code></h4>\n<p>This API is <em>optional</em>. If you provide this hypercore will call <code>.open</code> and wait for the callback to be called before calling any other methods.</p>\n<h4 id=\"-storage-read-offset-length-cb-\"><code>storage.read(offset, length, cb)</code></h4>\n<p>This API is <em>required</em>. Hypercore calls this when it wants to read data. You should return a buffer with length <code>length</code> that way read at the corresponding offset. If you cannot read this buffer call the callback with an error.</p>\n<h4 id=\"-storage-write-offset-buffer-cb-\"><code>storage.write(offset, buffer, cb)</code></h4>\n<p>This API is <em>required</em>. Hypercore calls this when it wants to write data. You should write the buffer at the corresponding offset and call the callback afterwards. If there was an error writing you should call the callback with that error.</p>\n<h4 id=\"-storage-close-cb-\"><code>storage.close(cb)</code></h4>\n<p>This API is <em>optional</em>. Hypercore will call this method when the feed is closing.</p>\n<h2 id=\"license\">License</h2>\n<p>MIT</p>\n"}})
  css('/Users/joe/node_modules/dat-docs/assets/styles.css', { global: true })
  app.start('#choo-root')
  