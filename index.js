
  var css = require('sheetify')
  var minidocs = require('minidocs')
  var app = minidocs({"title":"Dat Data","logo":"dat-data-logo.png","contents":[{"depth":1,"name":"Introduction"},{"depth":2,"name":"Welcome to Dat","key":"welcome","link":"/welcome"},{"depth":2,"name":"How Dat Works","key":"how-dat-works","link":"/how-dat-works"},{"depth":1,"name":"Specification"},{"depth":2,"name":"hyperdrive spec","key":"hyperdrive_spec","link":"/hyperdrive_spec"},{"depth":2,"name":"sleep","key":"sleep","link":"/sleep"},{"depth":1,"name":"References"},{"depth":2,"name":"API","key":"api","link":"/api"},{"depth":2,"name":"DIY Dat","key":"diy-dat","link":"/diy-dat"},{"depth":1,"name":"Modules"},{"depth":2,"name":"Overview","key":"ecosystem","link":"/ecosystem"},{"depth":2,"name":"Interface"},{"depth":3,"name":"Dat Command Line","key":"dat","link":"/dat"},{"depth":3,"name":"dat.land","key":"dat.land","link":"/dat.land"},{"depth":3,"name":"Dat Desktop","key":"dat-desktop","link":"/dat-desktop"},{"depth":2,"name":"Core"},{"depth":3,"name":"Hyperdrive","key":"hyperdrive","link":"/hyperdrive"},{"depth":3,"name":"Hypercore","key":"hypercore","link":"/hypercore"}],"markdown":"/Users/joe/node_modules/dat-docs/docs","initial":"welcome","basedir":"","dir":"/Users/joe/node_modules/dat-docs","routes":{"index":"/","welcome":"/welcome/","how-dat-works":"/how-dat-works/","hyperdrive_spec":"/hyperdrive_spec/","sleep":"/sleep/","api":"/api/","diy-dat":"/diy-dat/","ecosystem":"/ecosystem/","dat":"/dat/","dat.land":"/dat.land/","dat-desktop":"/dat-desktop/","hyperdrive":"/hyperdrive/","hypercore":"/hypercore/"},"html":{"welcome":"<h1 id=\"dat\">dat</h1>\n<p>Dat is a decentralized data tool for distributing data small and large.</p>\n<p><a href=\"http://webchat.freenode.net/?channels=dat\"><img src=\"https://img.shields.io/badge/irc%20channel-%23dat%20on%20freenode-blue.svg\" alt=\"#dat IRC channel on freenode\"></a>\n<a href=\"https://gitter.im/datproject/discussions?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\"><img src=\"https://badges.gitter.im/Join%20Chat.svg\" alt=\"datproject/discussions\"></a>\n<a href=\"http://docs.dat-data.com\"><img src=\"https://img.shields.io/badge/Dat%20Project-Docs-green.svg\" alt=\"docs\"></a></p>\n<h2 id=\"about-dat\">About Dat</h2>\n<p>Documentation for the Dat project is available at <a href=\"http://docs.dat-data.com\">docs.dat-data.com</a>.</p>\n<h3 id=\"key-features-\">Key features:</h3>\n<ul>\n<li><strong>Live sync</strong> folders by sharing files as they are added to the folder.</li>\n<li><strong>Distribute large files</strong> without copying data to a central server by connecting directly to peers.</li>\n<li><strong>Intelligently sync</strong> by deduplicating data between versions.</li>\n<li><strong>Verify data integrity</strong> using strong cryptographic hashes.</li>\n<li><strong>Work everywhere</strong>, including in the <a href=\"https://github.com/datproject/dat.land\">browser</a> and on the <a href=\"https://github.com/juliangruber/dat-desktop\">desktop</a>.</li>\n</ul>\n<p>Dat embraces the Unix philosophy: a modular design with composable parts. All of the pieces can be replaced with alternative implementations as long as they implement the abstract API.</p>\n<h3 id=\"ways-to-use-dat\">Ways to Use Dat</h3>\n<ul>\n<li><a href=\"https://github.com/maxogden/dat\">Dat CLI</a>: command line tool</li>\n<li><a href=\"https://github.com/juliangruber/dat-desktop/\">Dat Desktop</a>: desktop application</li>\n<li><a href=\"https://github.com/datproject/dat.land\">dat.land</a>: website application</li>\n</ul>\n","how-dat-works":"<h1 id=\"how-dat-works\">How Dat Works</h1>\n<p>Note this is about Dat 1.0 and later. For historical info about earlier incarnations of Dat (Alpha, Beta) check out <a href=\"http://dat-data.com/blog/2016-01-19-brief-history-of-dat\">this post</a>.</p>\n<p>When someone starts downloading data with the <a href=\"https://github.com/maxogden/dat\">Dat command-line tool</a>, here&#39;s what happens:</p>\n<h2 id=\"phase-1-source-discovery\">Phase 1: Source discovery</h2>\n<p>Dat links look like this: <code>dat.land/c3fcbcdcf03360529b47df32ccfb9bc1d7f64aaaa41cca43ca9ac7f6778db8da</code>. The domain, dat.land, is there so if someone opens the link in a browser we can provide them with download instructions, and as an easy way for people to visually distinguish and remember Dat links. Dat itself doesn&#39;t actually use the dat.land part, it just needs the last part of the link which is a fingerprint of the data that is being shared. The first thing that happens when you go to download data using one of these links is you ask various discovery networks if they can tell you where to find sources that have a copy of the data you need.</p>\n<p>Source discovery means finding the IP and port of all the known data sources online that have a copy of that data you are looking for. You can then connect to them and begin exchanging data. By introducing this discovery phase we are able to create a network where data can be discovered even if the original data source disappears.</p>\n<p>The discovery protocols we use are <a href=\"https://en.wikipedia.org/wiki/Name_server\">DNS name servers</a>, <a href=\"https://en.wikipedia.org/wiki/Multicast_DNS\">Multicast DNS</a> and the <a href=\"https://en.wikipedia.org/wiki/Mainline_DHT\">Kademlia Mainline Distributed Hash Table</a> (DHT). Each one has pros and cons, so we combine all three to increase the speed and reliability of discovering data sources.</p>\n<p>We run a <a href=\"https://www.npmjs.com/package/dns-discovery\">custom DNS server</a> that Dat clients use (in addition to specifying their own if they need to), as well as a <a href=\"https://github.com/bittorrent/bootstrap-dht\">DHT bootstrap</a> server. These discovery servers are the only centralized infrastructure we need for Dat to work over the Internet, but they are redundant, interchangeable, never see the actual data being shared, and anyone can run their own and Dat will still work even if they all go down. If this happens discovery will just be manual (e.g. manually sharing IP/ports). Every data source that has a copy of the data also advertises themselves across these discovery networks.</p>\n<p>The discovery logic itself is handled by a module that we wrote called <a href=\"http://npmjs.org/discovery-channel\">discovery-channel</a>, which wraps other modules we wrote to implement DNS and DHT logic into a single interface. We can give the Dat link we want to download to discovery-channel and we will get back all the sources it finds across the various discovery networks.</p>\n<h2 id=\"phase-2-source-connections\">Phase 2: Source connections</h2>\n<p>Up until this point we have just done searches to find who has the data we need. Now that we know who should talk to, we have to connect to them. We use either <a href=\"https://en.wikipedia.org/wiki/Transmission_Control_Protocol\">TCP</a> or <a href=\"https://en.wikipedia.org/wiki/Micro_Transport_Protocol\">UTP</a> sockets for the actual peer to peer connections. UTP is nice because it is designed to <em>not</em> take up all available bandwidth on a network (e.g. so that other people sharing your wifi can still use the Internet). We then layer on our own file sharing protocol on top, called <a href=\"https://github.com/mafintosh/hypercore\">Hypercore</a>. We also are working on WebRTC support so we can incorporate Browser and Electron clients for some really open web use cases.</p>\n<p>When we get the IP and port for a potential source we try to connect using all available protocols (currently TCP and sometimes UTP) and hope one works. If one connects first, we abort the other ones. If none connect, we try again until we decide that source is offline or unavailable to use and we stop trying to connect to them. Sources we are able to connect to go into a list of known good sources, so that if our Internet connection goes down we can use that list to reconnect to our good sources again quickly.</p>\n<p>If we get a lot of potential sources we pick a handful at random to try and connect to and keep the rest around as additional sources to use later in case we decide we need more sources. A lot of these are parameters that we can tune for different scenarios later, but have started with some best guesses as defaults.</p>\n<p>The connection logic is implemented in a module called <a href=\"https://www.npmjs.com/package/discovery-swarm\">discovery-swarm</a>. This builds on discovery-channel and adds connection establishment, management and statistics. You can see stats like how many sources are currently connected, how many good and bad behaving sources you&#39;ve talked to, and it automatically handles connecting and reconnecting to sources for you. Our UTP support is implemented in the module <a href=\"https://www.npmjs.com/package/utp-native\">utp-native</a>.</p>\n<h2 id=\"phase-3-data-exchange\">Phase 3: Data exchange</h2>\n<p>So now we have found data sources, have connected to them, but we havent yet figured out if they <em>actually</em> have the data we need. This is where our file transfer protocol <a href=\"https://www.npmjs.com/package/hyperdrive\">Hyperdrive</a> comes in.</p>\n<p>The short version of how Hyperdrive works is: It breaks file contents up in to pieces, hashes each piece and then constructs a <a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle tree</a> out of all of the pieces. This ultimately gives us the Dat link, which is the top level hash of the Merkle tree.</p>\n<p>Here&#39;s the long version:</p>\n<p>Hyperdrive shares and synchronizes a set of files, similar to rsync or Dropbox. For each file in the drive we use a technique called Rabin fingerprinting to break the file up into pieces. Rabin fingerprints are a specific strategy for what is called Content Defined Chunking. Here&#39;s an example:</p>\n<p><img src=\"https://raw.githubusercontent.com/datproject/docs/master/assets/cdc.png\" alt=\"cdc diagram\"></p>\n<p>We have configured our Rabin chunker to produce chunks that are around 16KB on average. So if you share a folder containing a single 1MB JPG you will get around 64 chunks.</p>\n<p>After feeding the file contents through the chunker, we take the chunks and calculate the SHA256 hash of each one. We then arrange these hashes into a special data structure we developed that we call the Flat In-Order Merkle Tree.</p>\n<h3 id=\"flat-in-order-merkle-tree\">Flat In-Order Merkle Tree</h3>\n<pre><code>      <span class=\"hljs-number\">3</span>\n  <span class=\"hljs-number\">1</span>       <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">0</span>   <span class=\"hljs-number\">2</span>   <span class=\"hljs-number\">4</span>   <span class=\"hljs-number\">6</span>\n</code></pre><p>Want to go lower level? Check out <a href=\"hyperdrive.md#how-hypercore-works\">How Hypercore Works</a></p>\n<p>When two peers connect to each other and begin speaking the Hyperdrive protocol they can efficiently determine if they have chunks the other one wants, and begin exchanging those chunks directly. Hyperdrive gives us the flexibility to have random access to any portion of a file while still verifying the other side isnt sending us bad data. We can also download different sections of files in parallel across all of the sources simultaneously, which increases overall download speed dramatically.</p>\n<h2 id=\"phase-4-data-archiving\">Phase 4: Data archiving</h2>\n<p>So now that you&#39;ve discovered, connected, and downloaded a copy of some data you can stick around for a while and serve up copies of the data to others who come along and want to download it.</p>\n<p>The first phase, source discovery, is actually an ongoing process. When you first search for data sources you only get the sources available at the time you did your search, so we make sure to perform discovery searches as often is practically possible to make sure new sources can be found and connected to.</p>\n<p>Every user of Dat is a source as long as they have 1 or more chunks of data. Just like with other decentralized file sharing protocols you will notice Dat may start uploading data before it finishes downloading.</p>\n<p>If the original source who shared the data goes offline it&#39;s OK, as long as other sources are available. As part of the mission as a not-for-profit we will be working with various institutions to ensure there are always sources available to accept new copies of data and stay online to serve those copies for important datasets such as scientific research data, open government data etc.</p>\n<p>Because Dat is built on a foundation of strong cryptographic data integrity and content addressable storage it gives us the possibility of implementing some really interesting version control techniques in the future. In that scenario archival data sources could choose to offer more disk space and archive every version of a Dat repository, whereas normal Dat users might only download and share one version that they happen to be interested in.</p>\n<h2 id=\"implementations\">Implementations</h2>\n<p>This covered a lot of ground. If you want to go deeper and see the implementations we are using in the <a href=\"https://github.com/maxogden/dat\">Dat command-line tool</a>, go to the <a href=\"ecosystem\">Dependencies</a> page</p>\n","hyperdrive_spec":"<h1 id=\"hyperdrive-hypercore-specification\">Hyperdrive + Hypercore Specification</h1>\n<h2 id=\"draft-version-1\">DRAFT Version 1</h2>\n<p>Hyperdrive is the peer-to-peer data distribution protocol that powers Dat. It consists of two parts. First there is hypercore which is the core protocol and swarm that handles distributing append-only logs of any binary data. The second part is hyperdrive which adds a filesystem specific protocol on top of hypercore.</p>\n<h2 id=\"hypercore\">Hypercore</h2>\n<p>The goal of hypercore is to distribute append-only logs across a network of peers. Peers download parts of the logs from other peers and can choose to only download the parts of a log they care about. Logs can contain arbitrary binary data payloads.</p>\n<p>A core goal is to be as simple and pragmatic as possible. This allows for easier implementations of clients which is an often overlooked property when implementing distributed systems. First class browser support is also an important goal as p2p data sharing in browsers is becoming more viable every day as WebRTC matures.</p>\n<p>It also tries to be modular and export responsibilities to external modules whenever possible. Peer discovery is a good example of this as it handled by 3rd party modules that wasn&#39;t written with hyperdrive in mind. A benefit of this is a much smaller core implementation that can focus on smaller and simpler problems.</p>\n<p>Prioritized synchronization of parts of a feed is also at the heart of hyperdrive as this allows for fast streaming with low latency of data such as structured datasets (wikipedia, genomic datasets), linux containers, audio, videos, and much more. To allow for low latency streaming another goal is also to keep verifiable block sizes as small as possible - even with huge data feeds.</p>\n<p>The protocol itself draws heavy inspiration from existing file sharing systems such as BitTorrent and <a href=\"https://datatracker.ietf.org/doc/rfc7574/?include_text=1\">PPSP</a></p>\n<h2 id=\"how-hypercore-works\">How Hypercore works</h2>\n<h3 id=\"flat-in-order-trees\">Flat In-Order Trees</h3>\n<p>A Flat In-Order Tree is a simple way represent a binary tree as a list. It also allows you to identify every node of a binary tree with a numeric index. Both of these properties makes it useful in distributed applications to simplify wire protocols that uses tree structures.</p>\n<p>Flat trees are described in <a href=\"https://datatracker.ietf.org/doc/rfc7574/?include_text=1\">PPSP RFC 7574 as &quot;Bin numbers&quot;</a> and a node version is available through the <a href=\"https://github.com/mafintosh/flat-tree\">flat-tree</a> module.</p>\n<p>A sample flat tree spanning 4 blocks of data looks like this:</p>\n<pre><code><span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>The even numbered entries represent data blocks (leaf nodes) and odd numbered entries represent parent nodes that have two children.</p>\n<p>The depth of an tree node can be calculated by counting the number of trailing 1s a node has in binary notation.</p>\n<pre><code><span class=\"hljs-symbol\">5 </span>in binary = <span class=\"hljs-number\">101</span> (one trailing <span class=\"hljs-number\">1</span>)\n<span class=\"hljs-symbol\">3 </span>in binary = <span class=\"hljs-number\">011</span> (two trailing <span class=\"hljs-number\">1</span>s)\n<span class=\"hljs-symbol\">4 </span>in binary = <span class=\"hljs-number\">100</span> (zero trailing <span class=\"hljs-number\">1</span>s)\n</code></pre><p>1 is the parent of (0, 2), 5 is the parent of (4, 6), and 3 is the parent of (1, 5).</p>\n<p>If the number of leaf nodes is a power of 2 the flat tree will only have a single root.</p>\n<p>Otherwise it&#39;ll have more than one. As an example here is a tree with 6 leafs:</p>\n<pre><code><span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n\n<span class=\"hljs-number\">8</span>\n  <span class=\"hljs-number\">9</span>\n<span class=\"hljs-number\">10</span>\n</code></pre><p>The roots spanning all the above leafs are 3 an 9. Throughout this document we&#39;ll use following tree terminology:</p>\n<ul>\n<li><code>parent</code> - a node that has two children (odd numbered)</li>\n<li><code>leaf</code> - a node with no children (even numbered)</li>\n<li><code>sibling</code> - the other node with whom a node has a mutual parent</li>\n<li><code>uncle</code> - a parent&#39;s sibling</li>\n</ul>\n<h2 id=\"merkle-trees\">Merkle Trees</h2>\n<p>A merkle tree is a binary tree where every leaf is a hash of a data block and every parent is the hash of both of its children.</p>\n<p>Merkle trees are useful for ensuring the integrity of content.</p>\n<p>Let&#39;s look at an example. Assume we have 4 data blocks, <code>(a, b, c, d)</code> and let <code>h(x)</code> be a hash function (the hyperdrive stack uses sha256 per default).</p>\n<p>Using flat-tree notation the merkle tree spanning these data blocks looks like this:</p>\n<pre><code><span class=\"hljs-number\">0</span> = h(a)\n  <span class=\"hljs-number\">1</span> = h(<span class=\"hljs-number\">0</span> + <span class=\"hljs-number\">2</span>)\n<span class=\"hljs-number\">2</span> = h(b)\n    <span class=\"hljs-number\">3</span> = h(<span class=\"hljs-number\">1</span> + <span class=\"hljs-number\">5</span>)\n<span class=\"hljs-number\">4</span> = h(c)\n  <span class=\"hljs-number\">5</span> = h(<span class=\"hljs-number\">4</span> + <span class=\"hljs-number\">6</span>)\n<span class=\"hljs-number\">6</span> = h(d)\n</code></pre><p>An interesting property of merkle trees is that the node 3 hashes the entire data set. Therefore we only need to trust node 3 to verify all data. However as we learned above there will only be a single root if there is a power of two data blocks.</p>\n<p>Again lets expand our data set to contain 6 items <code>(a, b, c, d, e, f)</code>:</p>\n<pre><code><span class=\"hljs-number\">0</span> = h(a)\n  <span class=\"hljs-number\">1</span> = h(<span class=\"hljs-number\">0</span> + <span class=\"hljs-number\">2</span>)\n<span class=\"hljs-number\">2</span> = h(b)\n    <span class=\"hljs-number\">3</span> = h(<span class=\"hljs-number\">1</span> + <span class=\"hljs-number\">5</span>)\n<span class=\"hljs-number\">4</span> = h(c)\n  <span class=\"hljs-number\">5</span> = h(<span class=\"hljs-number\">4</span> + <span class=\"hljs-number\">6</span>)\n<span class=\"hljs-number\">6</span> = h(d)\n\n<span class=\"hljs-number\">8</span> = h(e)\n  <span class=\"hljs-number\">9</span> = h(<span class=\"hljs-number\">8</span> + <span class=\"hljs-number\">10</span>)\n<span class=\"hljs-number\">10</span> = h(f)\n</code></pre><p>To ensure always have only a single root we&#39;ll simply hash all the roots together again. At most there will be <code>log2(number of data blocks)</code>.</p>\n<p>In addition to hashing the roots we&#39;ll also include a bin endian uint64 binary representation of the corresponding node index.</p>\n<p>Using the two above examples the final hashes would be:</p>\n<pre><code>hash1 = h(<span class=\"hljs-name\">uint64be</span>(<span class=\"hljs-name\">#3</span>) + <span class=\"hljs-number\">3</span>)\nhash2 = h(<span class=\"hljs-name\">uint64be</span>(<span class=\"hljs-name\">#9</span>) + <span class=\"hljs-number\">9</span> + uint64be(<span class=\"hljs-name\">#3</span>) + <span class=\"hljs-number\">3</span>)\n</code></pre><p>Each of these hashes can be used to fully verify each of the trees. Let&#39;s look at another example. Assume we trust <code>hash1</code> and another person wants to send block <code>0</code> to us. To verify block <code>0</code> the other person would also have to send the sibling hash and uncles until it reaches a root and the other missing root hashes. For the first tree that would mean hashes <code>(2, 5)</code>.</p>\n<p>Using these hashes we can reproduce <code>hash1</code> in the following way:</p>\n<pre><code><span class=\"hljs-number\">0</span> = h(block received)\n  <span class=\"hljs-number\">1</span> = h(<span class=\"hljs-number\">0</span> + <span class=\"hljs-number\">2</span>)\n<span class=\"hljs-number\">2</span> = (hash received)\n    <span class=\"hljs-number\">3</span> = h(<span class=\"hljs-number\">1</span> + <span class=\"hljs-number\">5</span>)\n  <span class=\"hljs-number\">5</span> = (hash received)\n</code></pre><p>If <code>h(uint64be(#3) + 3) == hash1</code> then we know that data we received from the other person is correct. They sent us <code>a</code> and the corresponding hashes.</p>\n<p>Since we only need uncle hashes to verify the block the amount of hashes we need is at worst <code>log2(number-of-blocks)</code> and the roots of the merkle trees which has the same complexity.</p>\n<p>A merkle tree generator is available on npm through the <a href=\"https://github.com/mafintosh/merkle-tree-stream\">merkle-tree-stream</a> module.</p>\n<h2 id=\"merkle-tree-deduplication\">Merkle Tree Deduplication</h2>\n<p>Merkle trees have another great property. They make it easy to deduplicate content that is similar.</p>\n<p>Assume we have two similar datasets:</p>\n<pre><code>(<span class=\"hljs-selector-tag\">a</span>, <span class=\"hljs-selector-tag\">b</span>, c, d, e)\n(<span class=\"hljs-selector-tag\">a</span>, <span class=\"hljs-selector-tag\">b</span>, c, d, f)\n</code></pre><p>These two datasets are the same except their last element is different. When generating merkle trees for the two data sets you&#39;d get two different root hashes out.</p>\n<p>However if we look a the flat-tree notation for the two trees:</p>\n<pre><code><span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n\n<span class=\"hljs-number\">8</span>\n</code></pre><p>We&#39;ll notice that the hash stored at 3 will be the same for both trees since the first four blocks are the same. Since we also send uncle hashes when sending a block of data we&#39;ll receive the hash for 3 when we request any block. If we maintain a simple index that maps a hash into the range of data it covers we can detect that we already have the data spanning 3 and we won&#39;t need to re-download that from another person.</p>\n<pre><code><span class=\"hljs-symbol\">1 </span>-&gt; (a, b)\n<span class=\"hljs-symbol\">3 </span>-&gt; (a, b, c, d)\n<span class=\"hljs-symbol\">5 </span>-&gt; (c, d)\n</code></pre><p>This means that two datasets share a similar sequence of data the merkle tree helps you detect that.</p>\n<h2 id=\"signed-merkle-trees\">Signed Merkle Trees</h2>\n<p>As described above the top hash of a merkle tree is the hash of all its content. This has both advantages and disadvanteges.</p>\n<p>An advantage is that you can always reproduce a merkle tree simply by having the data contents of a merkle tree.</p>\n<p>A disadvantage is every time you add content to your data set your merkle tree hash changes and you&#39;ll need to re-distribute the new hash.</p>\n<p>Using a bit of cryptography however we can make our merkle tree appendable. First generate a cryptographic key pair that can be used to sign data using <a href=\"https://ed25519.cr.yp.to/\">ed25519</a> keys, as they are compact in size (32 byte public keys). A key pair (public key, secret key) can be used to sign data. Signing data means that if you trust a public key and you receive data and a signature for that data you can verify that a signature was generated with the corresponding secret key.</p>\n<p>How does this relate to merkle trees? Instead of distributing the hash of a merkle tree we can distribute our public key instead. We then use our secret key to continously sign the merkle trees of our data set every time we append to it.</p>\n<p>Assume we have a data set with only a single item in it <code>(a)</code> and a key pair <code>(secret, public)</code>:</p>\n<pre><code>(<span class=\"hljs-name\">a</span>)\n</code></pre><p>We generate a merkle tree for this data set which will have the roots <code>0</code> and sign the hash of these roots (see the merkle tree section) with our secret key.</p>\n<p>If we want to send <code>a</code> to another person and they trust our public key we simply send <code>a</code> and the uncles needed to generate the roots plus our signature.</p>\n<p>If we append a new item to our data set we simply do the same thing:</p>\n<pre><code>(<span class=\"hljs-name\">a</span>, b)\n</code></pre><p>Notice that all new signatures verify the entire dataset since they all sign a merkle tree that spans all data. This serves two purposes. First of all it makes sure that the dataset publisher cannot change old data. It also ensures that the publisher cannot share different versions of the same dataset to different persons without the other people noticing it (at some point they&#39;ll get a signature for the same node index that has different hashes if they talk to multiple people).</p>\n<p>This technique has the added benefit that you can always convert a signed merkle tree to a normal unsigned one if you wish (or turn an unsigned tree into a signed tree).</p>\n<p>In general you should send as wide as possible signed tree back when using signed merkle trees as that lowers the amount of signatures the other person needs to verify which has a positive performance impact for some platforms. It will also allow other users to more quickly detect if a tree has duplicated content.</p>\n<h2 id=\"block-tree-digest\">Block Tree Digest</h2>\n<p>When asking for a block of data we want to reduce the amount of duplicate hashes that are sent back.</p>\n<p>In the merkle tree example for from earlier we ended up sending two hashes <code>(2, 5)</code> to verify block <code>0</code>.</p>\n<pre><code><span class=\"hljs-comment\">// If we trust 3 then 2 and 5 are needed to verify 0</span>\n\n<span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>Now if we ask for block <code>1</code> afterwards (<code>2</code> in flat tree notation) the other person doesn&#39;t need to send us any new hashes since we already received the hash for <code>2</code> when fetching block <code>0</code>.</p>\n<p>If we only use non-signed merkle trees the other person can easily calculate which hashes we already have if we tell them which blocks we&#39;ve got.</p>\n<p>This however isn&#39;t always possible if we use a signed merkle tree since the roots are changing. In general it also useful to be able to communicate that you have some hashes already without disclosing all the blocks you have.</p>\n<p>To communicate which hashes we have just have to communicate two things: which uncles we have and whether or not we have any parent node that can verify the tree.</p>\n<p>Looking at the above tree that means if we want to fetch block <code>0</code> we need to communicate whether of not we already have the uncles <code>(2, 5)</code> and the parent <code>3</code>. This information can be compressed into very small bit vector using the following scheme.</p>\n<p>Let the trailing bit donate whether or not the leading bit is a parent and not a uncle. Let the previous trailing bits denote wheather or not we have the next uncle.</p>\n<p>For example for block <code>0</code> the following bit vector <code>1011</code> is decoded the following way</p>\n<pre><code>// <span class=\"hljs-keyword\">for</span> <span class=\"hljs-keyword\">block</span> <span class=\"hljs-number\">0</span>\n\n<span class=\"hljs-number\">101</span>(<span class=\"hljs-number\">1</span>) &lt;<span class=\"hljs-comment\">-- tell us that the last bit is a parent and not an uncle</span>\n<span class=\"hljs-number\">10</span>(<span class=\"hljs-number\">1</span>)<span class=\"hljs-number\">1</span> &lt;<span class=\"hljs-comment\">-- we already have the first uncle, 2 so don't send us that</span>\n<span class=\"hljs-number\">1</span>(<span class=\"hljs-number\">0</span>)<span class=\"hljs-number\">11</span> &lt;<span class=\"hljs-comment\">-- we don't have the next uncle, 5</span>\n(<span class=\"hljs-number\">1</span>)<span class=\"hljs-number\">000</span> &lt;<span class=\"hljs-comment\">-- the final bit so this is parent. we have the next parent, 3</span>\n</code></pre><p>So using this digest the person can easily figure out that they only need to send us one hash, <code>5</code>, for us to verify block <code>0</code>.</p>\n<p>The bit vector <code>1</code> (only contains a single one) means that we already have all the hashes we need so just send us the block.</p>\n<p>These digests are very compact in size, only <code>(log2(number-of-blocks) + 2) / 8</code> bytes needed in the worst case. For example if you are sharing one trillion blocks of data the digest would be <code>(log2(1000000000000) + 2) / 8 ~= 6</code> bytes long.</p>\n<h3 id=\"bitfield-run-length-encoding\">Bitfield Run length Encoding</h3>\n<p>(talk about rle)</p>\n<h3 id=\"basic-privacy\">Basic Privacy</h3>\n<p>(talk about the privacy features + discovery key here)</p>\n<h2 id=\"hypercore-feeds\">Hypercore Feeds</h2>\n<p>(talk about how we use the above concepts to create a feed of data)</p>\n<h2 id=\"hypercore-replication-protocol\">Hypercore Replication Protocol</h2>\n<p>Lets assume two peers have the identifier for a hypercore feed. This could either be the hash of the merkle tree roots described above or a public key if they want to share a signed merkle tree. The two peers wants to exchange the data verified by this tree. Lets assume the two peers have somehow connected to each other.</p>\n<p>Hypercore uses a message based protocol to exchange data. All messages sent are encoded to binary values using Protocol Buffers. Protocol Buffers are a widely supported schema based encoding support. A Protocol Buffers implementation is available on npm through the <a href=\"https://github.com/mafintosh/protocol-buffers\">protocol-buffers</a> module.</p>\n<p>These are the types of messages the peers send to each other</p>\n<h4 id=\"open\">Open</h4>\n<p>This should be the first message sent and is also the only message without a type. It looks like this</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Open</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> feed = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> nonce = <span class=\"hljs-number\">2</span>;\n}\n</code></pre>\n<p>The <code>feed</code> should be set to the discovery key of the Merkle Tree as specified above. The <code>nonce</code> should be set to 24 bytes of high entropy random data. When running in encrypted mode this is the only message sent unencrypted.</p>\n<p>When you are done using a channel send an empty message to indicate end-of-channel.</p>\n<h4 id=\"-0-handshake\"><code>0</code> Handshake</h4>\n<p>The message contains the protocol handshake. It has type <code>0</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Handshake</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> id = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">repeated</span> <span class=\"hljs-built_in\">string</span> extensions = <span class=\"hljs-number\">2</span>;\n}\n</code></pre>\n<p>You should send this message after sending an open message. By sending it after an open message it will be encrypted and we wont expose our peer id to a third party. The current protocol version is 0.</p>\n<h4 id=\"-1-have\"><code>1</code> Have</h4>\n<p>You can send a have message to give the other peer information about which blocks of data you have. It has type <code>1</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Have</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> start = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> end = <span class=\"hljs-number\">2</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bytes</span> bitfield = <span class=\"hljs-number\">3</span>;\n}\n</code></pre>\n<p>If using a bitfield it should be encoded using a run length encoding described above. It is a good idea to send a have message soon as possible if you have blocks to share to reduce latency.</p>\n<h4 id=\"-2-want\"><code>2</code> Want</h4>\n<p>You can send a have message to give the other peer information about which blocks of data you want to have. It has type <code>2</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Want</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> start = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> end = <span class=\"hljs-number\">2</span>;\n}\n</code></pre>\n<p>You should only send the want message if you are interested in a section of the feed that the other peer has not told you about.</p>\n<h4 id=\"-3-request\"><code>3</code> Request</h4>\n<p>Send this message to request a block of data. You can request a block by block index or byte offset. If you are only interested\nin the hash of a block you can set the hash property to true. The nodes property can be set to a tree digest of the tree nodes you already\nhave for this block or byte range. A request message has type <code>3</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Request</span> </span>{\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> block = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> <span class=\"hljs-built_in\">bytes</span> = <span class=\"hljs-number\">2</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bool</span> hash = <span class=\"hljs-number\">3</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> nodes = <span class=\"hljs-number\">4</span>;\n}\n</code></pre>\n<h4 id=\"-4-data\"><code>4</code> Data</h4>\n<p>Send a block of data to the other peer. You can use this message to reply to a request or optimistically send other blocks of data to the other client. It has type <code>4</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Data</span> </span>{\n  <span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Node</span> </span>{\n    <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> index = <span class=\"hljs-number\">1</span>;\n    <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> size = <span class=\"hljs-number\">2</span>;\n    <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> hash = <span class=\"hljs-number\">3</span>;\n  }\n\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> block = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bytes</span> value = <span class=\"hljs-number\">2</span>;\n  <span class=\"hljs-keyword\">repeated</span> Node nodes = <span class=\"hljs-number\">3</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bytes</span> signature = <span class=\"hljs-number\">4</span>;\n}\n`\n</code></pre>\n<h4 id=\"-5-cancel\"><code>5</code> Cancel</h4>\n<p>Cancel a previous sent request. It has type <code>5</code>.</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Cancel</span> </span>{\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> block = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint64</span> <span class=\"hljs-built_in\">bytes</span> = <span class=\"hljs-number\">2</span>;\n}\n</code></pre>\n<h4 id=\"-6-pause\"><code>6</code> Pause</h4>\n<p>An empty message that tells the other peer that they should stop requesting new blocks of data. It has type <code>6</code>.</p>\n<h4 id=\"-7-resume\"><code>7</code> Resume</h4>\n<p>An empty message that tells the other peer that they can continue requesting new blocks of data. It has type <code>7</code>.</p>\n","sleep":"<h1 id=\"sleep-data-format\">SLEEP Data Format</h1>\n<h3 id=\"syncable-lightweight-event-emitting-persistence\">Syncable Lightweight Event Emitting Persistence</h3>\n<h3 id=\"version-2-0\">Version 2.0</h3>\n<p>SLEEP is a metadata format that allows a set of files to be accessed randomly, cryptographically verified, and dynamically updated. A SLEEP file contains content addressed file metadata in a representation specifically designed to allow partial streaming access to individual chunks of data. SLEEP files can be shared as a single downloadable file for easy distribution and we also specify a way to expose SLEEP over REST.</p>\n<p>The SLEEP format can be used in a similar way to how MD5 checksums are used over HTTP today, to verify the integrity of data downloads. Whereas MD5 or SHA are usually checksums of the whole data set, meaning consumers have to download the entire all available data before they are able to verify the integrity of any of it, SLEEP allows a set of data to be split in to many small pieces, each one getting it&#39;s own cryptographically secure checksum. This allows consumers to download subsets metadata and data, in whatever order they prefer, but allowing them to verify the integrity of each piece of data as it is accessed. It also includes cryptographic signatures allowing users to verify that data they received was created using a holder of a specific private key.</p>\n<h2 id=\"registers\">Registers</h2>\n<p>SLEEP is designed around the concept of a register, an append only list that you can trust. The contents of a register are cryptographically fingerprinted and an aggregate checksum can be used to verify the contents of the register have not been tampered with. There are various ways to calculate these aggregate checksums but the data in a register is a binary append only feed, e.g. an list of buffers that can only be updated by placing new buffers at the end of the list.</p>\n<p>SLEEP also provides an index that allows each piece of data in a register to be accessed randomly. In order to look up a specific piece of data in the register, you only need a small subset of the metadata in order to find it, making SLEEP suitable for live streaming or sparse download use cases.</p>\n<p>The register index is a Merkle tree where the leaf nodes are the hashes of the buffers in the register, and the rest of the nodes in the tree are derived Merkle hashes. A Merkle tree is defined as a tree where leaf nodes are a hash of some piece of data, and the rest of the nodes are the result of a hash of the concatenation of that nodes children.</p>\n<p>So, given a register with four values:</p>\n<pre><code><span class=\"hljs-bullet\">1. </span>a\n<span class=\"hljs-bullet\">2. </span>b\n<span class=\"hljs-bullet\">3. </span>c\n<span class=\"hljs-bullet\">4. </span>d\n</code></pre><p>To construct the register itself you concatenate all buffers, in this case resulting in &#39;abcd&#39;.</p>\n<p>The register index is constructed by creating a Merkle tree where the leaf nodes are the hash of our four values, and the rest of the nodes are the hash of the nodes two children hashes concatenated together.</p>\n<pre><code>hash(<span class=\"hljs-name\">a</span>)\n      &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">a</span>) + hash(<span class=\"hljs-name\">b</span>))\nhash(<span class=\"hljs-name\">b</span>)\n              &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">a</span>) + hash(<span class=\"hljs-name\">b</span>)) + hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">c</span>) + hash(<span class=\"hljs-name\">d</span>)))\nhash(<span class=\"hljs-name\">c</span>)\n      &gt; hash(<span class=\"hljs-name\">hash</span>(<span class=\"hljs-name\">c</span>) + hash(<span class=\"hljs-name\">d</span>))\nhash(<span class=\"hljs-name\">d</span>)\n</code></pre><p>To be able to refer to a specific node in the tree we use an in-order node traversal to assign integers to the nodes:</p>\n<pre><code><span class=\"hljs-number\">0</span>\n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span>\n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span>\n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>In-order node numbering has the property with our trees that leaf nodes are always even and non-leaf nodes are always odd. This can be used as a quick way to identify whether a node is a leaf or not.</p>\n<p>Every serialized node in the tree is one of two fixed widths, leaf nodes are all the same size and non-leaf nodes are the same size. When serializing the tree you simply write the nodes in order and concatenate them. Then to access a node by its in-order position you simply multiply the node length by the position to get the byte offset.</p>\n<p>All leaf nodes contain these two pieces of information:</p>\n<ul>\n<li>The sha256 hash of the data described by this node</li>\n<li>The absolute byte offset to the end of the region of data described by the node</li>\n</ul>\n<p>All non-leaf nodes contain these three pieces of information:</p>\n<ul>\n<li>The sha256 hash of the concatenation of the two children hashes</li>\n<li>The cryptographic signature of the hash</li>\n<li>The span of bytes that the the nodes children cover</li>\n</ul>\n<p>When initializing a register an asymmetric Ed25519 keypair is derived. The private key is never shared. The public key is used as the URL for the register. When signing hashes in the tree the public key is used to generate an EdDSA signature. For the example register above, &#39;abcd&#39;, the register index (in JSON) would be:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> keys = {\n  public: <span class=\"hljs-string\">'cc0cf6eeb82ca946ca60265ce0863fb2b3e3075ae25cba14d162ef20e3f9f223'</span>,\n  private: <span class=\"hljs-string\">'87399f90815db81e687efe4fd9fc60af336f4d9ae560fda106f94cb7a92a8804cc0cf6eeb82ca946ca60265ce0863fb2b3e3075ae25cba14d162ef20e3f9f223'</span>\n}\n\n<span class=\"hljs-keyword\">var</span> index = {\n  <span class=\"hljs-comment\">// sha256 of children[0].hash + children[1].hash</span>\n  hash: <span class=\"hljs-string\">'0440c655d63fec5c02cffd5d9b42d146aca03b255102b9b44b51c6a919b31351'</span>,\n  signature: <span class=\"hljs-string\">'1713dfbaf4a7f288003394b72ec486aa4fa1a837aa0b08662b3a14b63381b84c2e6965e2638fb5375ae2e92b47c2ab8718ec1914778518fcb3c0563eb2c09604'</span>,\n  span: <span class=\"hljs-number\">4</span>,\n  children: [\n    {\n      <span class=\"hljs-comment\">// echo -n \"$(echo -n \"a\" | shasum -a 256)$(echo -n \"b\" | shasum -a 256)\" | shasum -a 256</span>\n      hash: <span class=\"hljs-string\">'9ad4d5608a7a40db60c35f255fad821b762a82de168b4f4ed477d5d899b11796'</span>,\n      signature: <span class=\"hljs-string\">'2714b99e305ce46aa6d24eb2888cf0cbde33ad4a8bcd08705b59882837bf1e482f8dcab2ae94c2359914b1fe92831bfc73af99f1c6b1f5eba47efc4efa32de0d'</span>,\n      span: <span class=\"hljs-number\">2</span>,\n      children: [\n        {\n          <span class=\"hljs-comment\">// echo -n \"a\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'ca978112ca1bbdcafac231b39a23dc4da786eff8147c4e72b9807785afee48bb'</span>,\n          endByte: <span class=\"hljs-number\">1</span>\n        },\n        {\n          <span class=\"hljs-comment\">// echo -n \"b\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'3e23e8160039594a33894f6564e1b1348bbd7a0088d42c4acb73eeaed59c009d'</span>,\n          endByte: <span class=\"hljs-number\">2</span>\n        }\n      ]\n    },\n    {\n      <span class=\"hljs-comment\">// echo -n \"$(echo -n \"c\" | shasum -a 256)$(echo -n \"d\" | shasum -a 256)\" | shasum -a 256</span>\n      hash: <span class=\"hljs-string\">'09114d1a8a78b5d091e492c524ad7f8e941f403db0a6d3d52d36f17b9a86ce1c'</span>,\n      signature: <span class=\"hljs-string\">'6ac5e25206f69f22612e9b58c14f9ae6738233a57ab7f6e10c1384c4e074f6c8c606edbd95a9c099a0120947866079e3d13ef66dd7d5ed1756a89a5e9032a20d'</span>,\n      span: <span class=\"hljs-number\">2</span>,\n      children: [\n        {\n          <span class=\"hljs-comment\">// echo -n \"c\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'2e7d2c03a9507ae265ecf5b5356885a53393a2029d241394997265a1a25aefc6'</span>,\n          endByte: <span class=\"hljs-number\">3</span>\n        },\n        {\n          <span class=\"hljs-comment\">// echo -n \"d\" | shasum -a 256</span>\n          hash: <span class=\"hljs-string\">'18ac3e7343f016890c510e93f935261169d9e3f565436429830faf0934f4f8e4'</span>,\n          endByte: <span class=\"hljs-number\">4</span>\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n<p>The above representation of the tree is in JSON. However due to the properties of the in-order node indexes we can represent the same data in a flat index while still allowing traversals.</p>\n<h1 id=\"file-format\">File format</h1>\n<p>SLEEP files should be named <code>sleep.dat</code> and have the following format:</p>\n<pre><code>&lt;<span class=\"hljs-keyword\">Header</span>&gt;&lt;Register Index<span class=\"hljs-attr\">...</span>&gt;&lt;Register <span class=\"hljs-built_in\">Data</span><span class=\"hljs-attr\">...</span>&gt;\n</code></pre><p>The format is a header followed by the register index. Order of the index is determined by an in-order node traversal. After the register index, the actual register entry data follows. The header length is variable width, prefixed with a varint. The Register Index is composed of fixed width metadata entries. The Register Data is composed of concatenated non-fixed width data pieces.</p>\n<h3 id=\"header-format\">Header format</h3>\n<pre><code><span class=\"hljs-section\">&lt;varint header-length&gt;</span><span class=\"hljs-section\">&lt;header protobuf&gt;</span>\n</code></pre><p>The header protobuf has this schema:</p>\n<pre><code class=\"lang-protobuf\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">message</span> <span class=\"hljs-title\">Header</span> </span>{\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">bytes</span> datLink = <span class=\"hljs-number\">1</span>;\n  <span class=\"hljs-keyword\">required</span> <span class=\"hljs-built_in\">uint64</span> entryCount = <span class=\"hljs-number\">2</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">bool</span> isSigned = <span class=\"hljs-number\">3</span>;\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">string</span> hashType = <span class=\"hljs-number\">4</span> [default = <span class=\"hljs-string\">\"sha256\"</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint32</span> hashLength = <span class=\"hljs-number\">5</span> [default = <span class=\"hljs-number\">32</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">string</span> signatureType = <span class=\"hljs-number\">6</span> [default = <span class=\"hljs-string\">\"ed25519\"</span>];\n  <span class=\"hljs-keyword\">optional</span> <span class=\"hljs-built_in\">uint32</span> signatureLength = <span class=\"hljs-number\">7</span> [default = <span class=\"hljs-number\">64</span>];\n}\n</code></pre>\n<h3 id=\"register-index-format\">Register Index format</h3>\n<p>For non-signed even (leaf) nodes:</p>\n<pre><code><span class=\"hljs-section\">&lt;8-byte-span-length&gt;</span><span class=\"hljs-section\">&lt;data-hash&gt;</span>\n</code></pre><p>The 8-byte-span-length is an unsigned big endian 64 bit integer that should be number of cumulative bytes encompassed by all of the leaf nodes underneath the current node.</p>\n<p>For signed even (leaf) nodes:</p>\n<pre><code><span class=\"hljs-section\">&lt;8-byte-span-length&gt;</span><span class=\"hljs-section\">&lt;data-hash-signature&gt;</span><span class=\"hljs-section\">&lt;data-hash&gt;</span>\n</code></pre><p>For odd (non-leaf) nodes:</p>\n<pre><code>&lt;<span class=\"hljs-number\">8</span>-<span class=\"hljs-keyword\">byte</span>-<span class=\"hljs-keyword\">end</span>-<span class=\"hljs-built_in\">offset</span>&gt;&lt;data-hash&gt;\n</code></pre><p>The 8-byte-end-offset is an unsigned big endian 64 bit integer that should be the absolute position in the file for the <strong>end</strong> of the piece data described by this node.</p>\n<h3 id=\"register-data\">Register Data</h3>\n<p>The last section of the file is the actual data pieces, unmodified and concatenated together in sequential order.</p>\n<p>For the example tree above, the Register Data section would simply be <code>abcd</code>.</p>\n<h2 id=\"example\">Example</h2>\n<p>Given a tree like this you might want to look up in a <code>meta.dat</code> file the metadata for a specific node:</p>\n<pre><code><span class=\"hljs-number\">0</span>  \n  <span class=\"hljs-number\">1</span>\n<span class=\"hljs-number\">2</span> \n    <span class=\"hljs-number\">3</span>\n<span class=\"hljs-number\">4</span> \n  <span class=\"hljs-number\">5</span>\n<span class=\"hljs-number\">6</span>\n</code></pre><p>If you wanted to look up the metadata for 3, you could read the third (or any!) entry from sleep.dat:</p>\n<p>First you have to read the varint at the beginning of the file so you know how big the header is:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> varint = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'varint'</span>) <span class=\"hljs-comment\">// https://github.com/chrisdickinson/varint</span>\n<span class=\"hljs-keyword\">var</span> headerLength = varint.decode(firstChunkOfFile)\n</code></pre>\n<p>Now you can read the header from the file</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> headerOffset = varint.encodingLength(headerLength)\n<span class=\"hljs-keyword\">var</span> headerEndOffset = headerOffset + headerLength\n<span class=\"hljs-keyword\">var</span> headerBytes = firstChunkOfFile.slice(headerOffset, headerEndOffset)\n</code></pre>\n<p>To decode the header use the protobuf schema. We can use the <a href=\"https://github.com/mafintosh/protocol-buffers\">protocol-buffers</a> module to do that.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> messages = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'protocol-buffers'</span>)(fs.readFileSync(<span class=\"hljs-string\">'meta.dat.proto'</span>))\n<span class=\"hljs-keyword\">var</span> header = messages.Header.decode(headerBytes)\n</code></pre>\n<p>Now we have all the configuration required to calculate an entry offset.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> entryNumber = <span class=\"hljs-number\">42</span>\n<span class=\"hljs-keyword\">var</span> entryOffset = headerEndOffset + entryNumber * (<span class=\"hljs-number\">8</span> + header.hashLength)\n</code></pre>\n<p>If you have a signed feed, you have to take into account the extra space required for the signatures in the even nodes.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> entryOffset = headerLength + entryNumber * (<span class=\"hljs-number\">8</span> + header.hashLength)\n                  + <span class=\"hljs-built_in\">Math</span>.floor(entryNumber / <span class=\"hljs-number\">2</span>) * header.signatureLength\n</code></pre>\n","api":"<h2 id=\"1-0-architecture-design\">1.0 Architecture Design</h2>\n<ul>\n<li>dat: command-line api</li>\n<li>dat-desk: desktop application</li>\n<li>hyperdrive: storage layer</li>\n<li>discovery-swarm: dat network swarm discovery mechanism</li>\n</ul>\n<h2 id=\"dat\">dat</h2>\n<p>Command-line interface for dat</p>\n<h4 id=\"-dat-share-dir-\"><code>dat share DIR</code></h4>\n<p>Create a new dat link for the contents of the given directory. Prints a URL, which is a unique public key feed. This public key feed can be appended to. </p>\n<h6 id=\"options\">Options</h6>\n<ul>\n<li><code>--append=URL</code>: Adds the new URL to the public key feed.</li>\n<li><code>--static</code>: Ensures that the URL cannot be appended to.</li>\n</ul>\n<h4 id=\"-dat-url-dir-\"><code>dat URL DIR</code></h4>\n<p>Downloads the link to the given directory, and then exits. </p>\n<h6 id=\"options\">Options</h6>\n<ul>\n<li><code>--seed</code>: Downloads the link to the given directory and opens up a server that seeds it to the dat peer network.</li>\n<li><code>--list</code>: Fetches the metadata for the link and prints out the file list in the console.</li>\n</ul>\n","diy-dat":"<h1 id=\"diy-dat\">DIY Dat</h1>\n<p>This document shows how to write your own compatible <code>dat</code> client using node modules.</p>\n<p>The three essential node modules are called <a href=\"https://npmjs.org/hyperdrive\">hyperdrive</a>, <a href=\"https://npmjs.org/hyperdrive-archive-swarm\">hyperdrive-archive-swarm</a> and <a href=\"https://npmjs.org/level\">level</a>. Hyperdrive does file synchronization and versioning, hyperdrive-archive-swarm does peer discovery over local networks and the Internet, and level provides a local LevelDB for storing metadata. More details are available in <a href=\"how-dat-works.md\">How Dat Works</a>. The <a href=\"https://npmjs.org/dat\">dat</a> module itself is just some code that combines these modules and wraps them in a command-line API.</p>\n<p>Here&#39;s the minimal code needed to download data from a dat:</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-comment\">// run this like: node thisfile.js 4c325f7874b4070blahblahetc</span>\n<span class=\"hljs-comment\">// the dat link someone sent us, we want to download the data from it</span>\n<span class=\"hljs-keyword\">var</span> link = <span class=\"hljs-keyword\">new</span> Buffer(process.argv[<span class=\"hljs-number\">2</span>], <span class=\"hljs-string\">'hex'</span>)\n\n<span class=\"hljs-keyword\">var</span> Hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> Swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive-archive-swarm'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level'</span>)\n<span class=\"hljs-keyword\">var</span> raf = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-file'</span>)\n<span class=\"hljs-keyword\">var</span> each = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'stream-each'</span>)\n\n<span class=\"hljs-keyword\">var</span> db = level(<span class=\"hljs-string\">'./dat.db'</span>)\n<span class=\"hljs-keyword\">var</span> drive = Hyperdrive(db)\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(link, {\n  file: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-keyword\">return</span> raf(path.join(self.dir, name))\n  }\n})\n<span class=\"hljs-keyword\">var</span> swarm = Swarm(archive)\n\narchive.open(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err</span>) </span>{\n  <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">console</span>.error(err)\n  each(archive.list({live: archive.live}), <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">data, next</span>) </span>{\n    <span class=\"hljs-keyword\">var</span> startBytes = self.stats.bytesDown\n    archive.download(data, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err</span>) </span>{\n      <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">console</span>.error(err)\n      <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'file downloaded'</span>, data.relname)\n      next()\n    })\n  }, done)\n})\n</code></pre>\n","ecosystem":"<p>If you want to go deeper and see the implementations we are using in the <a href=\"https://github.com/maxogden/dat\">Dat command-line tool</a>, here you go:</p>\n<ul>\n<li><a href=\"https://www.npmjs.com/package/dat\">dat</a> - the main command line tool that uses all of the below</li>\n<li><a href=\"https://www.npmjs.com/package/discovery-channel\">discovery-channel</a> - discover data sources</li>\n<li><a href=\"https://www.npmjs.com/package/discovery-swarm\">discovery-swarm</a> - discover and connect to sources</li>\n<li><a href=\"https://www.npmjs.com/package/hyperdrive\">hyperdrive</a> - The file sharing network dat uses to distribute files and data. A technical specification / discussion on how hyperdrive works is <a href=\"https://github.com/mafintosh/hyperdrive/blob/master/SPECIFICATION.md\">available here</a></li>\n<li><a href=\"https://www.npmjs.com/package/hypercore\">hypercore</a> - exchange low-level binary blocks with many sources</li>\n<li><a href=\"https://www.npmjs.com/package/bittorrent-dht\">bittorrent-dht</a> - use the Kademlia Mainline DHT to discover sources</li>\n<li><a href=\"https://www.npmjs.com/package/dns-discovery\">dns-discovery</a> - use DNS name servers and Multicast DNS to discover sources</li>\n<li><a href=\"https://www.npmjs.com/package/utp-native\">utp-native</a> - UTP protocol implementation</li>\n<li><a href=\"https://www.npmjs.com/package/rabin\">rabin</a> - Rabin fingerprinter stream</li>\n<li><a href=\"https://www.npmjs.com/package/merkle-tree-stream\">merkle-tree-stream</a> - Used to construct Merkle trees from chunks</li>\n</ul>\n","dat":"<h1 id=\"dat\">dat</h1>\n<p>Dat is a decentralized data tool for distributing data small and large.</p>\n<p><a href=\"http://webchat.freenode.net/?channels=dat\"><img src=\"https://img.shields.io/badge/irc%20channel-%23dat%20on%20freenode-blue.svg\" alt=\"#dat IRC channel on freenode\"></a>\n<a href=\"https://gitter.im/datproject/discussions?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\"><img src=\"https://badges.gitter.im/Join%20Chat.svg\" alt=\"datproject/discussions\"></a>\n<a href=\"http://docs.dat-data.com\"><img src=\"https://readthedocs.org/projects/dat-cli/badge/?version=latest\" alt=\"docs\"></a></p>\n<table>\n<thead>\n<tr>\n<th>Windows</th>\n<th>Mac/Linux</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://ci.appveyor.com/project/maxogden/dat\"><img src=\"https://ci.appveyor.com/api/projects/status/github/maxogden/dat?branch=master&amp;svg=true\" alt=\"Build status\"></a></td>\n<td><a href=\"https://travis-ci.org/maxogden/dat\"><img src=\"https://api.travis-ci.org/maxogden/dat.svg\" alt=\"Travis\"></a></td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"about-dat\">About Dat</h2>\n<p>Documentation for the Dat project is available at <a href=\"http://docs.dat-data.com\">docs.dat-data.com</a>.</p>\n<h3 id=\"key-features-\">Key features:</h3>\n<ul>\n<li><strong>Live sync</strong> folders by sharing files as they are added to the folder.</li>\n<li><strong>Distribute large files</strong> without copying data to a central server by connecting directly to peers.</li>\n<li><strong>Intelligently sync</strong> by deduplicating data between versions.</li>\n<li><strong>Verify data integrity</strong> using strong cryptographic hashes.</li>\n<li><strong>Work everywhere</strong>, including in the <a href=\"https://github.com/datproject/dat.land\">browser</a> and on the <a href=\"https://github.com/juliangruber/dat-desktop\">desktop</a>.</li>\n</ul>\n<p>Dat embraces the Unix philosophy: a modular design with composable parts. All of the pieces can be replaced with alternative implementations as long as they implement the abstract API.</p>\n<h3 id=\"ways-to-use-dat\">Ways to Use Dat</h3>\n<ul>\n<li><a href=\"https://github.com/maxogden/dat\">Dat CLI</a>: command line tool</li>\n<li><a href=\"https://github.com/juliangruber/dat-desktop/\">Dat Desktop</a>: desktop application</li>\n<li><a href=\"https://github.com/datproject/dat.land\">dat.land</a>: website application</li>\n</ul>\n<h2 id=\"cli-development-status\">CLI Development Status</h2>\n<p>This is the Dat CLI 1.0 release candidate (RC2). We are actively seeking feedback &amp; developing this release candidate. Follow <a href=\"https://github.com/datproject/projects/issues/5\">this issue</a> for the Dat CLI road map discussion and see <a href=\"https://github.com/maxogden/dat/issues/486\">known RC2 issues</a>.</p>\n<p><strong>Please note</strong> that previous versions of Dat (alpha, beta) are incompatible with the 1.0 release candidate.</p>\n<h2 id=\"getting-started\">Getting started</h2>\n<h3 id=\"install\">Install</h3>\n<p>To install the 1.0 release candidate from npm:</p>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> dat -g\n</code></pre><p>If you receive an <code>EACCES</code> error read <a href=\"https://docs.npmjs.com/getting-started/fixing-npm-permissions\">this guide</a>.</p>\n<h3 id=\"using-dat\">Using Dat</h3>\n<p>There are two main commands in dat:</p>\n<ol>\n<li>Share data: <code>dat &lt;directory&gt;</code></li>\n<li>Download data: <code>dat &lt;dat-link&gt; &lt;download-directory&gt;</code></li>\n</ol>\n<h3 id=\"sharing-files\">Sharing Files</h3>\n<p>Share a directory by typing <code>dat &lt;directory&gt;</code>:</p>\n<pre><code>$ dat my_data/\nInitializing Dat in my_data/\n[DONE] readme.txt (<span class=\"hljs-number\">0.30</span> kB)\n[DONE] data.csv (<span class=\"hljs-number\">1.14</span> kB)\nItem<span class=\"hljs-variable\">s:</span> <span class=\"hljs-number\">2</span>  Size: <span class=\"hljs-number\">1.44</span> kB\nShare Link <span class=\"hljs-number\">4</span>f36c088e9687ddf53d36f785ab84c65f4d24d8c4161950519b96a57d65ae08a\nThe Share Link <span class=\"hljs-keyword\">is</span> secret <span class=\"hljs-built_in\">and</span> <span class=\"hljs-keyword\">only</span> those you share it with will <span class=\"hljs-keyword\">be</span> able <span class=\"hljs-keyword\">to</span> <span class=\"hljs-built_in\">get</span> the <span class=\"hljs-keyword\">files</span>\nSharing /Users/joe, connected <span class=\"hljs-keyword\">to</span> <span class=\"hljs-number\">2</span>/<span class=\"hljs-number\">4</span> sources\nUploading <span class=\"hljs-number\">28.62</span> kB/s, <span class=\"hljs-number\">765.08</span> kB Total\n</code></pre><p>You are now publishing that data from your computer. It will be publicly accessible as long as your terminal is open. The hash is a <strong>secret hash</strong>, your data is visible to anyone you send the hash to. As you add more files to the folder, dat will update and share the new files.</p>\n<h3 id=\"downloading-files\">Downloading Files</h3>\n<p>Your colleague can get data like this:</p>\n<pre><code>$ dat <span class=\"hljs-number\">2</span>bede435504c9482910b5d4e324e995a9bc4d6f068b98ae03d97e8d3ac5f80ea download_dir\nInitializing Dat from <span class=\"hljs-number\">52</span>d08a6d1ddc9b1f61b9862d2ae0d991676d489274bff6c5ebebecbfa3239f51\n[DONE] readme.txt (<span class=\"hljs-number\">0.30</span> kB)\n[DONE] data.csv (<span class=\"hljs-number\">1.14</span> kB)\n[DONE] <span class=\"hljs-number\">2</span> <span class=\"hljs-built_in\">items</span> (<span class=\"hljs-number\">1.44</span> kB)\nShare Link <span class=\"hljs-number\">52</span>d08a6d1ddc9b1f61b9862d2ae0d991676d489274bff6c5ebebecbfa3239f51\nThe Share Link <span class=\"hljs-keyword\">is</span> secret <span class=\"hljs-built_in\">and</span> <span class=\"hljs-keyword\">only</span> those you share it with will <span class=\"hljs-keyword\">be</span> able <span class=\"hljs-keyword\">to</span> <span class=\"hljs-built_in\">get</span> the <span class=\"hljs-keyword\">files</span>\nSyncing live updates, connected <span class=\"hljs-keyword\">to</span> <span class=\"hljs-number\">1</span>/<span class=\"hljs-number\">2</span> sources\nDownload Finished, you may <span class=\"hljs-keyword\">exit</span> process\n</code></pre><p>It will start downloading the data into the <code>download_dir</code> folder. Anyone who gets access to the unique dat-link will be able to download and re-host a copy of the data. It&#39;s distributed mad science!</p>\n<p>For more information, see the <a href=\"http://dat-cli.readthedocs.org/\">Dat CLI documentation</a> or the <a href=\"http://docs.dat-data.com\">dat project documentation</a>.</p>\n<h2 id=\"development\">Development</h2>\n<p>Please see <a href=\"https://github.com/maxogden/dat/blob/master/CONTRIBUTING.md\">guidelines on contributing</a> before submitting an issue or PR.</p>\n<h3 id=\"installing-from-source\">Installing from source</h3>\n<p>Clone this repository and in a terminal inside of the folder you cloned run this command:</p>\n<pre><code><span class=\"hljs-built_in\">npm</span> link\n</code></pre><p>This should add a <code>dat</code> command line command to your PATH. Now you can run the <code>dat</code> command to try it out.</p>\n<p>The contribution guide also has more tips on our <a href=\"https://github.com/maxogden/dat/blob/master/CONTRIBUTING.md#development-workflow\">development workflow</a>.</p>\n","dat.land":"<h1 id=\"dat-land\">dat.land</h1>\n<p>An online place for dats.</p>\n<p><a href=\"https://travis-ci.org/datproject/dat.land\"><img src=\"https://travis-ci.org/datproject/dat.land.svg?branch=master\" alt=\"Build Status\"></a></p>\n<p><a href=\"http://dat.land\">Try dat.land now</a></p>\n<h2 id=\"news\">News</h2>\n<p>We were recently awarded a <a href=\"http://www.knightfoundation.org/grants/201551933/\">$420,000 grant by the Knight Foundation</a> to get started on this project. It will be undergiong heavy development for the next few months.</p>\n<h3 id=\"develop\">develop</h3>\n<pre><code>npm <span class=\"hljs-keyword\">install</span>\n</code></pre><p>Do all of the below (watch assets and start server) in one command:</p>\n<pre><code>npm <span class=\"hljs-built_in\">run</span> dev\n</code></pre><p>Start the server:</p>\n<pre><code><span class=\"hljs-built_in\">npm</span> start\n</code></pre><p>To watch and build scss and javascript changes as you go (in a separate terminal):</p>\n<pre><code>npm <span class=\"hljs-built_in\">run</span> watch-css\nnpm <span class=\"hljs-built_in\">run</span> watch-js\n</code></pre><h3 id=\"build-for-production\">build for production</h3>\n<pre><code>npm <span class=\"hljs-built_in\">run</span> build\nnpm <span class=\"hljs-built_in\">run</span> minify\nnpm <span class=\"hljs-built_in\">run</span> <span class=\"hljs-built_in\">version</span>\n</code></pre><h3 id=\"using-shipit-for-deployment-and-install\">using shipit for deployment and install</h3>\n<p><a href=\"https://github.com/shipitjs/shipit\">shipit</a> and <a href=\"https://github.com/shipitjs/shipit-deploy\">shipit-deploy</a> depends on rsync version 3+, git version 1.7.8+, and OpenSSH version 5+. To upgrade rsync on a macosx machine, <a href=\"https://static.afp548.com/mactips/rsync.html\">follow instructions here</a> (see &quot;compile rsync 3.0.7&quot; section).</p>\n<p>install shipit-cli locally, globally:</p>\n<pre><code>npm <span class=\"hljs-keyword\">install </span><span class=\"hljs-keyword\">shipit-cli </span>-g\n</code></pre><p>the config file is <code>shipitfile.js</code>. you&#39;ll need to set the environment var <code>DATLAND_USER</code> in your local shell for it to know which account to use to access the server.</p>\n<p>to test your access to machine via shipit from your local command line, call shipit, then the environment (in this case <code>uat</code>, which is tracking the master branch), then the actual command which corresponds to tasks defined in the shipitfile:</p>\n<pre><code>shipit uat <span class=\"hljs-built_in\">pwd</span>\n</code></pre><p>to deploy and install a build on remote machine (note that shipit pulls build source from github, not your local project dir):</p>\n<pre><code>npm <span class=\"hljs-built_in\">run</span> deploy\n</code></pre>","dat-desktop":"<h1 id=\"dat-desktop\">dat-desktop</h1>\n<p>WIP desktop app for <a href=\"https://github.com/maxogden/dat\">dat</a>.</p>\n<p><img src=\"screenshot.png\" alt=\"\"></p>\n<p><a href=\"https://travis-ci.org/juliangruber/dat-desktop\"><img src=\"https://travis-ci.org/juliangruber/dat-desktop.svg?branch=master\" alt=\"Build Status\"></a></p>\n<p><a href=\"https://github.com/Flet/semistandard\"><img src=\"https://cdn.rawgit.com/flet/semistandard/master/badge.svg\" alt=\"js-semistandard-style\"></a></p>\n<h2 id=\"running\">Running</h2>\n<pre><code class=\"lang-bash\">$ npm install\n$ npm run rebuild\n$ npm start\n</code></pre>\n<h2 id=\"watch-and-compile-scss\">Watch and compile SCSS</h2>\n<pre><code class=\"lang-bash\">$ npm run watch-css\n</code></pre>\n<p>Then drop files onto the app window and watch the console.</p>\n<h2 id=\"cli\">CLI</h2>\n<p>-- <code>--data=DIR</code> overwrite the data path</p>\n<h2 id=\"styles\">Styles</h2>\n<p>For now, check out</p>\n<ul>\n<li><code>lib/render.js</code> for html</li>\n<li><code>/scss/main.scss</code> for scss</li>\n</ul>\n<p>Styles are imported from <a href=\"https://github.com/datproject/design\">https://github.com/datproject/design</a>. All variables, mixins, and component styles are available in main.</p>\n<p>There&#39;s also the html being generated by <a href=\"https://github.com/karissa/hyperdrive-ui\">hyperdrive-ui</a>.</p>\n<h2 id=\"license\">License</h2>\n<p>  MIT</p>\n","hyperdrive":"<h1 id=\"hyperdrive\">hyperdrive</h1>\n<p>A file sharing network based on <a href=\"https://github.com/maxogden/rabin\">rabin</a> file chunking and <a href=\"https://github.com/mafintosh/hypercore\">append only feeds of data verified by merkle trees</a>.</p>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> hyperdrive\n</code></pre><p><a href=\"http://travis-ci.org/mafintosh/hyperdrive\"><img src=\"http://img.shields.io/travis/mafintosh/hyperdrive.svg?style=flat\" alt=\"build status\"></a></p>\n<p>If you are interested in learning how hyperdrive works on a technical level a specification is available in the <a href=\"https://github.com/datproject/docs/blob/master/hyperdrive.md\">Dat docs repo</a></p>\n<h2 id=\"usage\">Usage</h2>\n<p>First create a new feed and share it</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level'</span>)\n<span class=\"hljs-keyword\">var</span> swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'discovery-swarm'</span>)()\n\n<span class=\"hljs-keyword\">var</span> db = level(<span class=\"hljs-string\">'./hyperdrive.db'</span>)\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(db)\n\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive()\n<span class=\"hljs-keyword\">var</span> ws = archive.createFileWriteStream(<span class=\"hljs-string\">'hello.txt'</span>) <span class=\"hljs-comment\">// add hello.txt</span>\n\nws.write(<span class=\"hljs-string\">'hello'</span>)\nws.write(<span class=\"hljs-string\">'world'</span>)\nws.end()\n\narchive.finalize(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{ <span class=\"hljs-comment\">// finalize the archive</span>\n  <span class=\"hljs-keyword\">var</span> link = archive.key.toString(<span class=\"hljs-string\">'hex'</span>)\n  <span class=\"hljs-built_in\">console</span>.log(link, <span class=\"hljs-string\">'&lt;-- this is your hyperdrive link'</span>)\n\n  <span class=\"hljs-comment\">// the archive is now ready for sharing.</span>\n  <span class=\"hljs-comment\">// we can use swarm to replicate it to other peers</span>\n  swarm.listen()\n  swarm.join(<span class=\"hljs-keyword\">new</span> Buffer(link, <span class=\"hljs-string\">'hex'</span>))\n  swarm.on(<span class=\"hljs-string\">'connection'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">connection</span>) </span>{\n    connection.pipe(archive.replicate()).pipe(connection)\n  })\n})\n</code></pre>\n<p>Then we can access the content from another process with the following code</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> swarm = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'discovery-swarm'</span>)()\n<span class=\"hljs-keyword\">var</span> hyperdrive = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hyperdrive'</span>)\n<span class=\"hljs-keyword\">var</span> level = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'level'</span>)\n\n<span class=\"hljs-keyword\">var</span> db = level(<span class=\"hljs-string\">'./another-hyperdrive.db'</span>)\n<span class=\"hljs-keyword\">var</span> drive = hyperdrive(db)\n\n<span class=\"hljs-keyword\">var</span> link = <span class=\"hljs-keyword\">new</span> Buffer(<span class=\"hljs-string\">'your-hyperdrive-link-from-the-above-example'</span>, <span class=\"hljs-string\">'hex'</span>)\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive(link)\n\nswarm.listen()\nswarm.join(link)\nswarm.on(<span class=\"hljs-string\">'connection'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">connection</span>) </span>{\n  connection.pipe(archive.replicate()).pipe(connection)\n  archive.get(<span class=\"hljs-number\">0</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err, entry</span>) </span>{ <span class=\"hljs-comment\">// get the first file entry</span>\n    <span class=\"hljs-built_in\">console</span>.log(entry) <span class=\"hljs-comment\">// prints {name: 'hello.txt', ...}</span>\n    <span class=\"hljs-keyword\">var</span> stream = archive.createFileReadStream(entry)\n    stream.on(<span class=\"hljs-string\">'data'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">data</span>) </span>{\n      <span class=\"hljs-built_in\">console</span>.log(data) <span class=\"hljs-comment\">// &lt;-- file data</span>\n    })\n    stream.on(<span class=\"hljs-string\">'end'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n      <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'no more data'</span>)\n    })\n  })\n})\n</code></pre>\n<p>If you want to write/read files to the file system provide a storage driver as the file option</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> raf = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'random-access-file'</span>) <span class=\"hljs-comment\">// a storage driver that writes to the file system</span>\n<span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  file: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-keyword\">return</span> raf(<span class=\"hljs-string\">'my-download-folder/'</span> + name)\n  }\n})\n</code></pre>\n<h2 id=\"api\">API</h2>\n<h4 id=\"-var-drive-hyperdrive-db-\"><code>var drive = hyperdrive(db)</code></h4>\n<p>Create a new hyperdrive instance. db should be a <a href=\"https://github.com/level/levelup\">levelup</a> instance.</p>\n<h4 id=\"-var-archive-drive-createarchive-key-options-\"><code>var archive = drive.createArchive([key], [options])</code></h4>\n<p>Creates an archive instance. If you want to download/upload an existing archive provide the archive key\nas the first argument. Options include</p>\n<pre><code class=\"lang-js\">{\n  live: <span class=\"hljs-literal\">false</span>, <span class=\"hljs-comment\">// set this to share the archive without finalizing it</span>\n  sparse: <span class=\"hljs-literal\">false</span>, <span class=\"hljs-comment\">// set this to only download the pieces of the feed you are requesting / prioritizing</span>\n  file: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-comment\">// set this to determine how file data is stored.</span>\n    <span class=\"hljs-comment\">// the storage instance should implement the hypercore storage api</span>\n    <span class=\"hljs-comment\">// https://github.com/mafintosh/hypercore#storage-api</span>\n    <span class=\"hljs-keyword\">return</span> someStorageInstance\n  }\n}\n</code></pre>\n<p>If you do not provide the file option all file data is stored in the leveldb.</p>\n<h4 id=\"-archive-key-\"><code>archive.key</code></h4>\n<p>A buffer that verifies the archive content. In live mode this is a 32 byte public key.\nOtherwise it is a 32 byte hash.</p>\n<h4 id=\"-archive-live-\"><code>archive.live</code></h4>\n<p>Boolean whether archive is live. <code>true</code> by default. Note that its only populated after archive.open(cb) has been fired.</p>\n<h4 id=\"-archive-append-entry-callback-\"><code>archive.append(entry, callback)</code></h4>\n<p>Append an entry to the archive. Only possible if this is an live archive you originally created\nor an unfinalized archive.</p>\n<p>If you set the file option in the archive constructor you can use this method to append an already\nexisting file to the archive.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> archive = drive.createArchive({\n  file: <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">name</span>) </span>{\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'returning storage for'</span>, name)\n    <span class=\"hljs-keyword\">return</span> raf(name)\n  }\n})\n\narchive.append(<span class=\"hljs-string\">'hello.txt'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'hello.txt was read and appended'</span>)\n})\n</code></pre>\n<h4 id=\"-archive-finalize-callback-\"><code>archive.finalize([callback])</code></h4>\n<p>Finalize the archive. You need to do this before sharing it if the archive is not live.</p>\n<h4 id=\"-archive-get-index-callback-\"><code>archive.get(index, callback)</code></h4>\n<p>Reads an entry from the archive.</p>\n<h4 id=\"-archive-download-index-callback-\"><code>archive.download(index, callback)</code></h4>\n<p>Fully downloads a file / entry from the archive and calls the callback afterwards.</p>\n<h4 id=\"-archive-close-callback-\"><code>archive.close([callback])</code></h4>\n<p>Closes and releases all resources used by the archive. Call this when you are done using it.</p>\n<h4 id=\"-archive-on-download-data-\"><code>archive.on(&#39;download&#39;, data)</code></h4>\n<p>Emitted every time a piece of data is downloaded</p>\n<h4 id=\"-archive-on-upload-data-\"><code>archive.on(&#39;upload&#39;, data)</code></h4>\n<p>Emitted every time a piece of data is uploaded</p>\n<h4 id=\"-var-rs-archive-list-opts-cb-\"><code>var rs = archive.list(opts={}, cb)</code></h4>\n<p>Returns a readable stream of all entries in the archive.</p>\n<ul>\n<li><code>opts.offset</code> - start streaming from this offset (default: 0)</li>\n<li><code>opts.live</code> - keep the stream open as new updates arrive (default: false)</li>\n</ul>\n<p>You can collect the results of the stream with <code>cb(err, entries)</code>.</p>\n<h4 id=\"-var-rs-archive-createfilereadstream-entry-options-\"><code>var rs = archive.createFileReadStream(entry, [options])</code></h4>\n<p>Returns a readable stream of the file content of an file in the archive.</p>\n<p>Options include:</p>\n<pre><code class=\"lang-js\">{\n  start: startOffset, <span class=\"hljs-comment\">// defaults to 0</span>\n  end: endOffset <span class=\"hljs-comment\">// defaults to file.length</span>\n}\n</code></pre>\n<h4 id=\"-var-ws-archive-createfilewritestream-entry-\"><code>var ws = archive.createFileWriteStream(entry)</code></h4>\n<p>Returns a writable stream that writes a new file to the archive. Only possible if the archive is live and you own it\nor if the archive is not finalized.</p>\n<h4 id=\"-var-cursor-archive-createbytecursor-entry-options-\"><code>var cursor = archive.createByteCursor(entry, [options])</code></h4>\n<p>Creates a cursor that can seek and traverse parts of the file.</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> cursor = archive.createByteCursor(<span class=\"hljs-string\">'hello.txt'</span>)\n\n<span class=\"hljs-comment\">// seek to byte offset 10000 and read the rest.</span>\ncursor.seek(<span class=\"hljs-number\">10000</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">err</span>) </span>{\n  <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">throw</span> err\n  cursor.next(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> <span class=\"hljs-title\">loop</span> (<span class=\"hljs-params\">err, data</span>) </span>{\n    <span class=\"hljs-keyword\">if</span> (err) <span class=\"hljs-keyword\">throw</span> err\n    <span class=\"hljs-keyword\">if</span> (!data) <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'no more data'</span>)\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'cursor.position is '</span> + cursor.position)\n    <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'read'</span>, data.length, <span class=\"hljs-string\">'bytes'</span>)\n    cursor.next(loop)\n  })\n})\n</code></pre>\n<p>Options include</p>\n<pre><code class=\"lang-js\">{\n  start: startOffset, <span class=\"hljs-comment\">// defaults to 0</span>\n  end: endOffset <span class=\"hljs-comment\">// defaults to file.length</span>\n}\n</code></pre>\n<h4 id=\"-var-stream-archive-replicate-\"><code>var stream = archive.replicate()</code></h4>\n<p>Pipe this stream together with another peer that is interested in the same archive to replicate the content.</p>\n<h2 id=\"license\">License</h2>\n<p>MIT</p>\n","hypercore":"<h1 id=\"hypercore\">hypercore</h1>\n<p>Hypercore is a protocol and p2p network for distributing and replicating feeds of binary data. It is the low level component that <a href=\"https://github.com/mafintosh/hyperdrive\">Hyperdrive</a> is built on top off.</p>\n<pre><code>npm <span class=\"hljs-keyword\">install</span> hypercore\n</code></pre><p><a href=\"http://travis-ci.org/mafintosh/hypercore\"><img src=\"http://img.shields.io/travis/mafintosh/hypercore.svg?style=flat\" alt=\"build status\"></a></p>\n<p>It runs both in the node and in the browser using <a href=\"https://github.com/substack/node-browserify\">browserify</a>.</p>\n<h2 id=\"usage\">Usage</h2>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> hypercore = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'hypercore'</span>)\n<span class=\"hljs-keyword\">var</span> net = <span class=\"hljs-built_in\">require</span>(<span class=\"hljs-string\">'net'</span>)\n\n<span class=\"hljs-keyword\">var</span> core = hypercore(db) <span class=\"hljs-comment\">// db is a leveldb instance</span>\n<span class=\"hljs-keyword\">var</span> feed = core.createFeed()\n\nfeed.append([<span class=\"hljs-string\">'hello'</span>, <span class=\"hljs-string\">'world'</span>], <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\"></span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'appended two blocks'</span>)\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'key is'</span>, feed.key.toString(<span class=\"hljs-string\">'hex'</span>))\n})\n\nfeed.on(<span class=\"hljs-string\">'upload'</span>, <span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">block, data</span>) </span>{\n  <span class=\"hljs-built_in\">console</span>.log(<span class=\"hljs-string\">'uploaded block'</span>, block, data)\n})\n\n<span class=\"hljs-keyword\">var</span> server = net.createServer(<span class=\"hljs-function\"><span class=\"hljs-keyword\">function</span> (<span class=\"hljs-params\">socket</span>) </span>{\n  socket.pipe(feed.replicate()).pipe(socket)\n})\n\nserver.listen(<span class=\"hljs-number\">10000</span>)\n</code></pre>\n<p>In another process</p>\n<pre><code class=\"lang-js\"><span class=\"hljs-keyword\">var</span> core = hypercore(anotherDb)\n<span class=\"hljs-keyword\">var</span> feed = core.createFeed(<span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">key-printed-out-above</span>&gt;</span>)\nvar socket = net.connect(10000)\n\nsocket.pipe(feed.replicate()).pipe(socket)\n\nfeed.on('download', function (block, data) {\n  console.log('downloaded block', block, data)\n})</span>\n</code></pre>\n<h2 id=\"api\">API</h2>\n<h4 id=\"-var-core-hypercore-db-\"><code>var core = hypercore(db)</code></h4>\n<p>Create a new hypercore instance. <code>db</code> should be a leveldb instance.</p>\n<h4 id=\"-var-feed-core-createfeed-key-options-\"><code>var feed = core.createFeed([key], [options])</code></h4>\n<p>Create a new feed. A feed stores a list of append-only data (buffers). A feed has a <code>.key</code> property that you can pass in to <code>createFeed</code> if you want to retrieve an old feed. Per default all feeds are appendable (live).</p>\n<p>Options include:</p>\n<pre><code class=\"lang-js\">{\n  live: <span class=\"hljs-literal\">true</span>,\n  storage: externalStorage,\n  sparse: <span class=\"hljs-literal\">false</span>\n}\n</code></pre>\n<p>Set <code>sparse</code> to <code>true</code> if you only want to download the pieces of the feed you are requesting / prioritizing. Otherwise the entire feed will be downloaded if nothing else is prioritized.</p>\n<p>If you want to create a static feed, one you cannot reappend data to, pass the <code>{live: false}</code> option.\nThe storage option allows you to store data outside of leveldb. This is very useful if you use hypercore to distribute files.</p>\n<p>See the <a href=\"#storage-api\">Storage API</a> section for more info</p>\n<h4 id=\"-var-stream-core-replicate-\"><code>var stream = core.replicate()</code></h4>\n<p>Create a generic replication stream. Use the <code>feed.replicate(stream)</code> API described below to replicate specific feeds of data.</p>\n<h4 id=\"-var-stream-core-list-options-callback-\"><code>var stream = core.list([options], [callback])</code></h4>\n<p>List all feed keys in the database. Optionally you can pass a callback to buffer them into an array. Options include:</p>\n<pre><code class=\"lang-js\">{\n  values: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// set this to get feed attributes, not just feed keys</span>\n}\n</code></pre>\n<h2 id=\"-feed-api-\"><code>Feed API</code></h2>\n<p>As mentioned above a feed stores a list of data for you that you can replicate to other peers. It has the following API</p>\n<h4 id=\"-feed-key-\"><code>feed.key</code></h4>\n<p>The key of this feed. A 32 byte buffer. Other peers need this key to start replicating the feed.</p>\n<h4 id=\"-feed-discoverykey-\"><code>feed.discoveryKey</code></h4>\n<p>A 32 byte buffer containing a discovery key of the feed. The discovery key is sha-256 hmac of the string <code>hypercore</code> using the feed key as the password.\nYou can use the discovery key to find other peers sharing this feed without disclosing your feed key to a third party.</p>\n<h4 id=\"-feed-blocks-\"><code>feed.blocks</code></h4>\n<p>The total number of known data blocks in the feed.</p>\n<h4 id=\"-feed-bytes-\"><code>feed.bytes</code></h4>\n<p>The total byte size of known data blocks in the feed.</p>\n<h4 id=\"-feed-open-cb-\"><code>feed.open(cb)</code></h4>\n<p>Call this method to ensure that a feed is opened. You do not need to call this but the <code>.blocks</code> property will not be populated until the feed has been opened.</p>\n<h4 id=\"-feed-append-data-callback-\"><code>feed.append(data, callback)</code></h4>\n<p>Append a block of data to the feed. If you want to append more than one block you can pass in an array.</p>\n<h4 id=\"-feed-get-index-callback-\"><code>feed.get(index, callback)</code></h4>\n<p>Retrieve a block of data from the feed.</p>\n<h4 id=\"-feed-prioritize-range-callback-\"><code>feed.prioritize(range, [callback])</code></h4>\n<p>Prioritize a range of blocks to download. Will call the callback when done.\nRange should look like this</p>\n<pre><code class=\"lang-js\">{\n  start: startBlock,\n  end: optionalEndBlock,\n  priority: <span class=\"hljs-number\">2</span> <span class=\"hljs-comment\">// a priority level spanning [0-5]</span>\n  linear: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// download the range linearly</span>\n}\n</code></pre>\n<h4 id=\"-feed-unprioritize-range-\"><code>feed.unprioritize(range)</code></h4>\n<p>Unprioritize a range.</p>\n<h4 id=\"-feed-seek-byteoffset-callback-\"><code>feed.seek(byteOffset, callback)</code></h4>\n<p>Find the block of data containing the byte offset. Calls the callback with <code>(err, index, offset)</code> where <code>index</code> is the block index and <code>offset</code> is the the relative byte offset in the block returned by <code>.get(index)</code>.</p>\n<h4 id=\"-feed-finalize-callback-\"><code>feed.finalize(callback)</code></h4>\n<p>If you are not using a live feed you need to call this method to finalize the feed once you are ready to share it.\nFinalizing will set the <code>.key</code> property and allow other peers to get your data.</p>\n<h4 id=\"-var-stream-feed-createwritestream-options-\"><code>var stream = feed.createWriteStream([options])</code></h4>\n<p>Create a writable stream that appends to the feed. If the feed is a static feed, it will be finalized when you end the stream.</p>\n<h4 id=\"-var-stream-feed-createreadstream-options-\"><code>var stream = feed.createReadStream([options])</code></h4>\n<p>Create a readable stream that reads from the feed. Options include:</p>\n<pre><code class=\"lang-js\">{\n  start: startIndex, <span class=\"hljs-comment\">// read from this index</span>\n  end: endIndex, <span class=\"hljs-comment\">// read until this index</span>\n  live: <span class=\"hljs-literal\">false</span> <span class=\"hljs-comment\">// set this to keep the read stream open</span>\n}\n</code></pre>\n<h4 id=\"-var-stream-feed-replicate-options-\"><code>var stream = feed.replicate([options])</code></h4>\n<p>Get a replication stream for this feed. Pipe this to another peer to start replicating this feed with another peer.\nIf you create multiple replication streams to multiple peers you&#39;ll upload/download data to all of them (meaning the load will spread out).</p>\n<p>Per default the replication stream encrypts all messages sent using the feed key and an incrementing nonce. This helps ensures that the remote peer also the feed key and makes it harder for a man-in-the-middle to sniff the data you are sending.</p>\n<p>Set <code>{private: false}</code> to disable this.</p>\n<p>Hypercore uses a simple multiplexed protocol that allows one replication stream to be used for multiple feeds at once.\nIf you want to join another replication stream simply pass it as the stream option</p>\n<pre><code class=\"lang-js\">feed.replicate({stream: anotherReplicationStream})\n</code></pre>\n<p>As a shorthand you can also do <code>feed.replicate(stream)</code>.</p>\n<h4 id=\"-stream-on-open-discoverykey-\"><code>stream.on(&#39;open&#39;, discoveryKey)</code></h4>\n<p>Emitted when a remote feed joins the replication stream and you haven&#39;t. You can use this as a signal to join the stream yourself if you want to.</p>\n<h4 id=\"-feed-on-download-block-data-\"><code>feed.on(&#39;download&#39;, block, data)</code></h4>\n<p>Emitted when a data block has been downloaded</p>\n<h4 id=\"-feed-on-download-finished-\"><code>feed.on(&#39;download-finished&#39;)</code></h4>\n<p>Emitted when all available data has been downloaded.\nWill re-fire when a live feed is updated and you download all the new blocks.</p>\n<h4 id=\"-feed-on-upload-block-data-\"><code>feed.on(&#39;upload&#39;, block, data)</code></h4>\n<p>Emitted when a data block has been uploaded</p>\n<h2 id=\"storage-api\">Storage API</h2>\n<p>If you want to use external storage to store the hypercore data (metadata will still be stored in the leveldb) you need to implement the following api and provide that as the <code>storage</code> option when creating a feed.</p>\n<p>Some node modules that implement this interface are</p>\n<ul>\n<li><a href=\"https://github.com/mafintosh/random-access-file\">random-access-file</a> Writes data to a file.</li>\n<li><a href=\"https://github.com/mafintosh/random-access-memory\">random-access-memory</a> Writes data to memory.</li>\n</ul>\n<h4 id=\"-storage-open-cb-\"><code>storage.open(cb)</code></h4>\n<p>This API is <em>optional</em>. If you provide this hypercore will call <code>.open</code> and wait for the callback to be called before calling any other methods.</p>\n<h4 id=\"-storage-read-offset-length-cb-\"><code>storage.read(offset, length, cb)</code></h4>\n<p>This API is <em>required</em>. Hypercore calls this when it wants to read data. You should return a buffer with length <code>length</code> that way read at the corresponding offset. If you cannot read this buffer call the callback with an error.</p>\n<h4 id=\"-storage-write-offset-buffer-cb-\"><code>storage.write(offset, buffer, cb)</code></h4>\n<p>This API is <em>required</em>. Hypercore calls this when it wants to write data. You should write the buffer at the corresponding offset and call the callback afterwards. If there was an error writing you should call the callback with that error.</p>\n<h4 id=\"-storage-close-cb-\"><code>storage.close(cb)</code></h4>\n<p>This API is <em>optional</em>. Hypercore will call this method when the feed is closing.</p>\n<h2 id=\"license\">License</h2>\n<p>MIT</p>\n"}})
  css('/Users/joe/node_modules/dat-docs/assets/styles.css', { global: true })
  app.start('#choo-root')
  